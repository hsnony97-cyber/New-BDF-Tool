#!/usr/bin/env python3
"""
Integrated BDF + RF Tool v12.1 - COMPLETE
==========================================
Full integration of BDF Tool v12 + RF Check Tool v2.1

Tab 1: BDF Merge Preparation
Tab 2: BDF Post-Process
Tab 3: BDF Offset Tool
Tab 4: RF Check Tool (Reserve Factor + Required Thickness)

Complete working version with all 3,000+ lines of code.
"""

import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import os
import re
import threading
import csv
import shutil
import tempfile
import pandas as pd
from pyNastran.bdf.bdf import BDF
from pyNastran.op2.op2 import OP2
import numpy as np


class IntegratedBDFRFTool:
    def __init__(self, root):
        self.root = root
        self.root.title("Integrated BDF + RF Tool v12.1")
        self.root.geometry("1100x950")
        
        # Tab 1 variables
        self.thermal_bdfs = []
        self.maneuver_bdfs = []
        self.excel_path = tk.StringVar()
        self.output_folder = tk.StringVar()
        self.output_thermal_name = tk.StringVar(value="merged_thermal.bdf")
        self.output_maneuver_name = tk.StringVar(value="merged_maneuver.bdf")
        self.set_id = tk.StringVar(value="99")
        self.temp_initial = tk.StringVar(value="10")
        
        # Tab 2 variables
        self.run_bdfs = []
        self.property_excel_path = tk.StringVar()
        self.nastran_path = tk.StringVar()
        self.run_output_folder = tk.StringVar()
        self.csv_output_name = tk.StringVar(value="bar_stress_results.csv")
        self.combined_csv_name = tk.StringVar(value="combined_stress_results.csv")
        self.tab2_offset_csv = tk.StringVar()  # Offset CSV for Tab 2
        
        self.bar_properties = {}
        self.skin_properties = {}
        self.residual_strength_df = None
        
        # Tab 3 variables (Offset Tool)
        self.offset_input_bdf = tk.StringVar()
        self.offset_element_excel = tk.StringVar()
        self.offset_csv_path = tk.StringVar()
        self.offset_output_name = tk.StringVar(value="offset_applied.bdf")
        self.offset_csv_name = tk.StringVar(value="calculated_offsets.csv")
        

        
        # ============================================================
        # Tab 4: RF Check Tool Variables
        # ============================================================
        self.rf_bar_stress_df = None
        self.rf_combined_stress_df = None
        self.rf_merged_combined_df = None
        self.rf_max_stress_df = None
        self.rf_max_stress_elem_df = None
        self.rf_allowable_df = None
        self.rf_allowable_interp = {}
        self.rf_allowable_elem_interp = {}  # Element-based curve fits
        self.rf_results_df = None
        self.rf_results_elem_df = None
        
        self.rf_bar_stress_path = tk.StringVar()
        self.rf_combined_stress_path = tk.StringVar()
        self.rf_allowable_path = tk.StringVar()

        # Tab 4 path variables (used in setup_tab4)
        self.bar_stress_path = tk.StringVar()
        self.combined_stress_path = tk.StringVar()
        self.allowable_path = tk.StringVar()
        self.rf_output_folder = tk.StringVar()
        self.rf_min_rf_var = tk.StringVar(value="1.0")
        self.rf_r2_threshold_var = tk.StringVar(value="0.95")
        self.rf_min_data_points_var = tk.StringVar(value="3")

        # Tab 4 settings variables (used in setup_tab4)
        self.min_rf_var = tk.StringVar(value="1.0")
        self.r2_threshold_var = tk.StringVar(value="0.95")
        self.min_data_points_var = tk.StringVar(value="3")

        self.setup_ui()
    
    def setup_ui(self):
        self.notebook = ttk.Notebook(self.root)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        self.tab1 = ttk.Frame(self.notebook)
        self.notebook.add(self.tab1, text="BDF Merge Preparation")
        self.setup_tab1()
        
        self.tab2 = ttk.Frame(self.notebook)
        self.notebook.add(self.tab2, text="BDF Post-Process")
        self.setup_tab2()
        
        self.tab3 = ttk.Frame(self.notebook)
        self.notebook.add(self.tab3, text="BDF Offset Tool")
        self.setup_tab3()
        
        self.tab4 = ttk.Frame(self.notebook)
        self.notebook.add(self.tab4, text="RF Check Tool")
        self.setup_tab4()
    
    def setup_tab1(self):
        main = ttk.Frame(self.tab1, padding="10")
        main.pack(fill=tk.BOTH, expand=True)
        
        ttk.Label(main, text="BDF Merge Preparation v8", font=('Helvetica', 14, 'bold')).pack(pady=(0,10))
        
        # === THERMAL SECTION ===
        thermal_main = ttk.LabelFrame(main, text="THERMAL", padding="5")
        thermal_main.pack(fill=tk.X, pady=5)
        
        thm_master_f = ttk.Frame(thermal_main)
        thm_master_f.pack(fill=tk.X, pady=2)
        ttk.Label(thm_master_f, text="MASTER BDFs:", width=12).pack(side=tk.LEFT)
        ttk.Button(thm_master_f, text="Add...", command=self.add_thermal_bdfs).pack(side=tk.LEFT, padx=2)
        ttk.Button(thm_master_f, text="Clear", command=self.clear_thermal_bdfs).pack(side=tk.LEFT, padx=2)
        self.thermal_count = tk.StringVar(value="0 files")
        ttk.Label(thm_master_f, textvariable=self.thermal_count).pack(side=tk.LEFT, padx=5)
        
        self.thermal_listbox = tk.Listbox(thermal_main, height=3, width=100)
        self.thermal_listbox.pack(fill=tk.X, pady=2)
        
        # === MANEUVER SECTION ===
        maneuver_main = ttk.LabelFrame(main, text="MANEUVER", padding="5")
        maneuver_main.pack(fill=tk.X, pady=5)
        
        man_master_f = ttk.Frame(maneuver_main)
        man_master_f.pack(fill=tk.X, pady=2)
        ttk.Label(man_master_f, text="MASTER BDFs:", width=12).pack(side=tk.LEFT)
        ttk.Button(man_master_f, text="Add...", command=self.add_maneuver_bdfs).pack(side=tk.LEFT, padx=2)
        ttk.Button(man_master_f, text="Clear", command=self.clear_maneuver_bdfs).pack(side=tk.LEFT, padx=2)
        self.maneuver_count = tk.StringVar(value="0 files")
        ttk.Label(man_master_f, textvariable=self.maneuver_count).pack(side=tk.LEFT, padx=5)
        
        self.maneuver_listbox = tk.Listbox(maneuver_main, height=3, width=100)
        self.maneuver_listbox.pack(fill=tk.X, pady=2)
        
        # === SETTINGS ===
        sf = ttk.LabelFrame(main, text="Settings", padding="10")
        sf.pack(fill=tk.X, pady=5)
        ttk.Label(sf, text="Excel:").grid(row=0, column=0, sticky=tk.W)
        ttk.Entry(sf, textvariable=self.excel_path, width=70).grid(row=0, column=1, padx=5)
        ttk.Button(sf, text="Browse", command=self.browse_excel).grid(row=0, column=2)
        ttk.Label(sf, text="Output:").grid(row=1, column=0, sticky=tk.W)
        ttk.Entry(sf, textvariable=self.output_folder, width=70).grid(row=1, column=1, padx=5)
        ttk.Button(sf, text="Browse", command=self.browse_output).grid(row=1, column=2)
        ttk.Label(sf, text="SET ID:").grid(row=2, column=0, sticky=tk.W)
        ttk.Entry(sf, textvariable=self.set_id, width=10).grid(row=2, column=1, sticky=tk.W, padx=5)
        ttk.Label(sf, text="TEMP(INIT):").grid(row=3, column=0, sticky=tk.W)
        ttk.Entry(sf, textvariable=self.temp_initial, width=10).grid(row=3, column=1, sticky=tk.W, padx=5)
        
        bf = ttk.Frame(main)
        bf.pack(fill=tk.X, pady=10)
        self.process_btn = ttk.Button(bf, text=">>> PROCESS & MERGE <<<", command=self.start_processing)
        self.process_btn.pack(side=tk.LEFT, padx=5)
        ttk.Button(bf, text="Clear Log", command=self.clear_log1).pack(side=tk.LEFT)
        
        self.progress1 = ttk.Progressbar(main, mode='indeterminate')
        self.progress1.pack(fill=tk.X, pady=5)
        
        lf = ttk.LabelFrame(main, text="Log", padding="10")
        lf.pack(fill=tk.BOTH, expand=True)
        self.log_text1 = scrolledtext.ScrolledText(lf, height=15)
        self.log_text1.pack(fill=tk.BOTH, expand=True)
    
    def setup_tab2(self):
        main = ttk.Frame(self.tab2, padding="10")
        main.pack(fill=tk.BOTH, expand=True)
        
        ttk.Label(main, text="BDF Post-Process", font=('Helvetica', 14, 'bold')).pack(pady=(0,10))
        
        bf = ttk.LabelFrame(main, text="BDF Files", padding="10")
        bf.pack(fill=tk.X, pady=5)
        bb = ttk.Frame(bf)
        bb.pack(fill=tk.X)
        ttk.Button(bb, text="Add...", command=self.add_run_bdfs).pack(side=tk.LEFT, padx=5)
        ttk.Button(bb, text="Clear", command=self.clear_run_bdfs).pack(side=tk.LEFT)
        self.run_listbox = tk.Listbox(bf, height=3, width=100)
        self.run_listbox.pack(fill=tk.X, pady=5)
        self.run_count = tk.StringVar(value="0 files")
        ttk.Label(bf, textvariable=self.run_count).pack(anchor=tk.W)
        
        pf = ttk.LabelFrame(main, text="Property Excel", padding="10")
        pf.pack(fill=tk.X, pady=5)
        ttk.Label(pf, text="Excel:").grid(row=0, column=0, sticky=tk.W)
        ttk.Entry(pf, textvariable=self.property_excel_path, width=70).grid(row=0, column=1, padx=5)
        ttk.Button(pf, text="Browse", command=self.browse_property_excel).grid(row=0, column=2)
        ttk.Button(pf, text="Load Properties", command=self.load_properties).grid(row=1, column=0, pady=5)
        
        pvf = ttk.Frame(pf)
        pvf.grid(row=2, column=0, columnspan=3, sticky=tk.EW, pady=5)
        self.bar_prop_text = tk.Text(pvf, height=2, width=35)
        self.bar_prop_text.pack(side=tk.LEFT, padx=3)
        self.bar_prop_text.insert(tk.END, "Bar: Not loaded")
        self.skin_prop_text = tk.Text(pvf, height=2, width=35)
        self.skin_prop_text.pack(side=tk.LEFT, padx=3)
        self.skin_prop_text.insert(tk.END, "Skin: Not loaded")
        self.resid_text = tk.Text(pvf, height=2, width=30)
        self.resid_text.pack(side=tk.LEFT, padx=3)
        self.resid_text.insert(tk.END, "Residual: Not loaded")
        
        nf = ttk.LabelFrame(main, text="Nastran", padding="10")
        nf.pack(fill=tk.X, pady=5)
        ttk.Label(nf, text="Path:").grid(row=0, column=0, sticky=tk.W)
        ttk.Entry(nf, textvariable=self.nastran_path, width=70).grid(row=0, column=1, padx=5)
        ttk.Button(nf, text="Browse", command=self.browse_nastran).grid(row=0, column=2)
        
        of = ttk.LabelFrame(main, text="Output", padding="10")
        of.pack(fill=tk.X, pady=5)
        ttk.Label(of, text="Folder:").grid(row=0, column=0, sticky=tk.W)
        ttk.Entry(of, textvariable=self.run_output_folder, width=70).grid(row=0, column=1, padx=5)
        ttk.Button(of, text="Browse", command=self.browse_run_output).grid(row=0, column=2)
        ttk.Label(of, text="CSV:").grid(row=1, column=0, sticky=tk.W)
        ttk.Entry(of, textvariable=self.csv_output_name, width=25).grid(row=1, column=1, sticky=tk.W, padx=5)
        
        # === OFFSET CSV SECTION ===
        off = ttk.LabelFrame(main, text="Apply Offsets (Optional)", padding="10")
        off.pack(fill=tk.X, pady=5)
        ttk.Label(off, text="Offset CSV:").grid(row=0, column=0, sticky=tk.W, padx=5)
        ttk.Entry(off, textvariable=self.tab2_offset_csv, width=60).grid(row=0, column=1, padx=5)
        ttk.Button(off, text="Browse", command=self.browse_tab2_offset_csv).grid(row=0, column=2, padx=5)
        self.btn_apply_offset_tab2 = ttk.Button(off, text=">>> APPLY OFFSETS TO BDFs <<<", 
                                                command=self.start_apply_offsets_tab2)
        self.btn_apply_offset_tab2.grid(row=1, column=0, columnspan=3, pady=10)
        
        af = ttk.Frame(main)
        af.pack(fill=tk.X, pady=10)
        self.btn1 = ttk.Button(af, text="1.Update Props", command=self.start_update_properties, width=14)
        self.btn1.pack(side=tk.LEFT, padx=2)
        self.btn2 = ttk.Button(af, text="2.Run Nastran", command=self.start_run_nastran, width=14)
        self.btn2.pack(side=tk.LEFT, padx=2)
        self.btn3 = ttk.Button(af, text="3.Post-Process", command=self.start_postprocess, width=14)
        self.btn3.pack(side=tk.LEFT, padx=2)
        self.btn4 = ttk.Button(af, text="4.Combine", command=self.start_combine_stress, width=12)
        self.btn4.pack(side=tk.LEFT, padx=2)
        self.btn_full = ttk.Button(af, text=">>> FULL <<<", command=self.start_full_run, width=12)
        self.btn_full.pack(side=tk.LEFT, padx=2)
        ttk.Button(af, text="Clear", command=self.clear_log2).pack(side=tk.LEFT, padx=2)
        
        self.progress2 = ttk.Progressbar(main, mode='indeterminate')
        self.progress2.pack(fill=tk.X, pady=5)
        
        lf = ttk.LabelFrame(main, text="Log", padding="10")
        lf.pack(fill=tk.BOTH, expand=True)
        self.log_text2 = scrolledtext.ScrolledText(lf, height=12)
        self.log_text2.pack(fill=tk.BOTH, expand=True)

    # ============= TAB 1 HELPERS =============
    def add_thermal_bdfs(self):
        files = filedialog.askopenfilenames(filetypes=[("BDF","*.bdf *.dat *.nas"),("All","*.*")])
        for f in files:
            if f not in self.thermal_bdfs:
                self.thermal_bdfs.append(f)
                self.thermal_listbox.insert(tk.END, f)
        self.thermal_count.set(f"{len(self.thermal_bdfs)} files")
    
    def clear_thermal_bdfs(self):
        self.thermal_bdfs.clear()
        self.thermal_listbox.delete(0, tk.END)
        self.thermal_count.set("0 files")
    
    def add_maneuver_bdfs(self):
        files = filedialog.askopenfilenames(filetypes=[("BDF","*.bdf *.dat *.nas"),("All","*.*")])
        for f in files:
            if f not in self.maneuver_bdfs:
                self.maneuver_bdfs.append(f)
                self.maneuver_listbox.insert(tk.END, f)
        self.maneuver_count.set(f"{len(self.maneuver_bdfs)} files")
    
    def clear_maneuver_bdfs(self):
        self.maneuver_bdfs.clear()
        self.maneuver_listbox.delete(0, tk.END)
        self.maneuver_count.set("0 files")
    
    def browse_excel(self):
        f = filedialog.askopenfilename(filetypes=[("Excel","*.xlsx *.xls")])
        if f: self.excel_path.set(f)
    
    def browse_output(self):
        f = filedialog.askdirectory()
        if f: self.output_folder.set(f)
    
    def log1(self, msg):
        self.log_text1.insert(tk.END, msg + "\n")
        self.log_text1.see(tk.END)
        self.root.update_idletasks()
    
    def clear_log1(self):
        self.log_text1.delete(1.0, tk.END)
    
    def format_include_nastran(self, abs_path):
        """Uzun INCLUDE path'lerini Nastran formatına uygun böler."""
        include_line = f"INCLUDE '{abs_path}'"
        if len(include_line) <= 72:
            return [include_line]
        parts = abs_path.split('/')
        lines = []
        current_line = "INCLUDE '"
        for i, part in enumerate(parts):
            is_last = (i == len(parts) - 1)
            segment = part if is_last else part + '/'
            if len(current_line + segment) <= 72:
                current_line += segment
            else:
                if current_line != "INCLUDE '":
                    lines.append(current_line)
                current_line = segment
        if current_line:
            current_line += "'"
            lines.append(current_line)
        return lines
    
    def read_file_safe(self, fpath):
        """Dosyayı güvenli şekilde oku."""
        for enc in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:
            try:
                with open(fpath, 'r', encoding=enc, errors='replace') as f:
                    return f.read()
            except:
                continue
        return ""
    
    def extract_subcase_load_info(self, bdf_content):
        """
        BDF içeriğinden TÜM SUBCASE bilgilerini çıkarır.
        Bir dosyada birden fazla SUBCASE olabilir (MASTER_GUST.BDF gibi).
        
        Returns: list of dicts, her biri:
            - subcase_id
            - load_id
            - temp_load_id
            - subtitle
        """
        results = []
        lines = bdf_content.split('\n')
        
        current_subcase = None
        current_load = None
        current_temp_load = None
        current_subtitle = None
        
        for line in lines:
            upper = line.upper().strip()
            original = line.strip()
            
            # SUBCASE satırı
            if upper.startswith('SUBCASE'):
                # Önceki subcase'i kaydet
                if current_subcase is not None:
                    results.append({
                        'subcase_id': current_subcase,
                        'load_id': current_load,
                        'temp_load_id': current_temp_load,
                        'subtitle': current_subtitle
                    })
                
                # Yeni subcase başlat
                parts = upper.split()
                if len(parts) >= 2:
                    try:
                        current_subcase = int(parts[1])
                        current_load = None
                        current_temp_load = None
                        current_subtitle = None
                    except:
                        pass
            
            # LOAD = satırı
            elif current_subcase and upper.startswith('LOAD') and '=' in upper:
                match = re.search(r'LOAD\s*=\s*(\d+)', upper)
                if match:
                    current_load = int(match.group(1))
            
            # TEMPERATURE(LOAD) = satırı
            elif current_subcase and 'TEMPERATURE' in upper and 'LOAD' in upper and '=' in upper:
                match = re.search(r'TEMPERATURE\s*\(\s*LOAD\s*\)\s*=\s*(\d+)', upper)
                if match:
                    current_temp_load = int(match.group(1))
            
            # SUBTITLE satırı
            elif current_subcase and upper.startswith('SUBTITLE'):
                m = re.search(r'SUBTITLE\s+(.+)', original, re.IGNORECASE)
                if m:
                    current_subtitle = m.group(1).strip()
            
            # BEGIN BULK'a ulaştıysak case control bitti
            elif upper.startswith('BEGIN') and 'BULK' in upper:
                break
        
        # Son subcase'i kaydet
        if current_subcase is not None:
            results.append({
                'subcase_id': current_subcase,
                'load_id': current_load,
                'temp_load_id': current_temp_load,
                'subtitle': current_subtitle
            })
        
        return results
    
    def parse_multiline_includes(self, content, bdf_dir):
        """
        Çok satırlı INCLUDE'ları parse eder.
        Nastran formatında INCLUDE şöyle olabilir:
        
        INCLUDE '../../../_COMMON_/INTERFACE/STRUCTURAL/INTER_AF_HT/
        INTER_AF_HT_STRU.BDF'
        
        Returns: list of dict with keys: lines, full_text, abs_path, start_idx, end_idx
        """
        lines = content.split('\n')
        includes = []
        i = 0
        
        while i < len(lines):
            line = lines[i]
            upper = line.upper().strip()
            
            if upper.startswith('INCLUDE'):
                # INCLUDE başladı - tırnak içindeki path'i bul
                include_lines = [line]
                full_text = line
                
                # Tırnak sayısını kontrol et - tek tırnak veya çift tırnak
                quote_char = None
                if "'" in line:
                    quote_char = "'"
                elif '"' in line:
                    quote_char = '"'
                
                if quote_char:
                    # Tırnak sayısını say
                    quote_count = full_text.count(quote_char)
                    
                    # Eğer tek tırnak varsa (açılmış ama kapanmamış), devam satırlarını oku
                    j = i + 1
                    while quote_count % 2 != 0 and j < len(lines):
                        next_line = lines[j]
                        include_lines.append(next_line)
                        full_text += '\n' + next_line
                        quote_count = full_text.count(quote_char)
                        j += 1
                    
                    # Path'i çıkar - newline'ları temizle
                    clean_text = full_text.replace('\n', '')
                    match = re.search(rf"INCLUDE\s*{quote_char}([^{quote_char}]*){quote_char}", 
                                     clean_text, re.IGNORECASE)
                    if match:
                        inc_path = match.group(1).strip()
                        # Absolute path'e çevir
                        if not os.path.isabs(inc_path):
                            abs_path = os.path.normpath(os.path.join(bdf_dir, inc_path))
                        else:
                            abs_path = os.path.normpath(inc_path)
                        abs_path = abs_path.replace('\\', '/')
                        
                        includes.append({
                            'lines': include_lines,
                            'full_text': full_text,
                            'abs_path': abs_path,
                            'start_idx': i,
                            'end_idx': j - 1 if j > i + 1 else i
                        })
                    
                    i = j
                else:
                    i += 1
            else:
                i += 1
        
        return includes
    
    def collect_all_lines_from_masters(self, bdf_files, load_case_set, common_type):
        """
        Tüm MASTER BDF'lerden TÜM SATIRLARI toplar.
        
        1. Excel'deki SUBCASE ID'lere uyan MASTER BDF'leri bulur
        2. Her birinden TÜM satırları alır ve alt alta yapıştırır
        3. INCLUDE'ları kategorize et:
           - COMMON LOAD/THERMAL → Ayrı tut (sonra INCLUDE olarak eklenecek)
           - Diğerleri (STRUCTURE, INTERFACE, vs.) → Satır olarak ekle (pyNastran açacak)
        4. Duplicate satırları çıkar
        
        NOT: Bir BDF birden fazla SUBCASE içerebilir (MASTER_GUST.BDF gibi)
        
        common_type: 'LOAD' veya 'THERMAL'
        
        Returns: (all_lines, common_includes, subcase_info_map)
        """
        all_lines_raw = []  # Tüm satırlar (INCLUDE satırları dahil - COMMON hariç)
        common_includes_raw = []  # COMMON INCLUDE path'leri
        subcase_info_map = {}
        processed_files = set()  # Aynı dosyayı birden fazla kez işlememek için
        
        self.log1(f"    Reading ALL lines from {len(bdf_files)} MASTER BDFs...")
        matched_count = 0
        matched_subcases = 0
        
        for bdf_path in bdf_files:
            bdf_dir = os.path.dirname(os.path.abspath(bdf_path))
            content = self.read_file_safe(bdf_path)
            
            # INREL dosyalarını atla
            bdf_basename = os.path.basename(bdf_path).upper()
            if 'INREL' in bdf_basename:
                self.log1(f"      SKIP (INREL): {os.path.basename(bdf_path)}")
                continue
            
            # TÜM SUBCASE bilgilerini al (birden fazla olabilir)
            all_subcases = self.extract_subcase_load_info(content)
            
            # Bu dosyadaki hangi subcase'ler Excel listesinde?
            matching_subcases = []
            for sc_info in all_subcases:
                sc_id = sc_info['subcase_id']
                if sc_id and sc_id in load_case_set:
                    matching_subcases.append(sc_info)
            
            # Eşleşen subcase varsa bu dosyayı işle
            if matching_subcases:
                # Dosya daha önce işlendiyse sadece subcase info'ları ekle
                if bdf_path in processed_files:
                    for sc_info in matching_subcases:
                        sc_id = sc_info['subcase_id']
                        if sc_id not in subcase_info_map:
                            if common_type == 'THERMAL':
                                subcase_info_map[sc_id] = {
                                    'temp_load_id': sc_info['temp_load_id'] if sc_info['temp_load_id'] else sc_id,
                                    'subtitle': sc_info['subtitle'] if sc_info['subtitle'] else f"Thermal Case {sc_id}"
                                }
                            else:
                                subcase_info_map[sc_id] = {
                                    'load_id': sc_info['load_id'] if sc_info['load_id'] else sc_id,
                                    'subtitle': sc_info['subtitle'] if sc_info['subtitle'] else f"Manoeuvre {sc_id}"
                                }
                            matched_subcases += 1
                    continue
                
                processed_files.add(bdf_path)
                matched_count += 1
                
                # Log - kaç subcase eşleşti
                sc_ids = [str(sc['subcase_id']) for sc in matching_subcases]
                self.log1(f"      MATCH: {os.path.basename(bdf_path)} ({len(matching_subcases)} subcases: {', '.join(sc_ids[:5])}{'...' if len(sc_ids) > 5 else ''})")
                
                # Subcase info'ları kaydet
                for sc_info in matching_subcases:
                    sc_id = sc_info['subcase_id']
                    matched_subcases += 1
                    if common_type == 'THERMAL':
                        subcase_info_map[sc_id] = {
                            'temp_load_id': sc_info['temp_load_id'] if sc_info['temp_load_id'] else sc_id,
                            'subtitle': sc_info['subtitle'] if sc_info['subtitle'] else f"Thermal Case {sc_id}"
                        }
                    else:
                        subcase_info_map[sc_id] = {
                            'load_id': sc_info['load_id'] if sc_info['load_id'] else sc_id,
                            'subtitle': sc_info['subtitle'] if sc_info['subtitle'] else f"Manoeuvre {sc_id}"
                        }
                
                # Önce tüm INCLUDE'ları parse et (çok satırlı dahil)
                all_includes = self.parse_multiline_includes(content, bdf_dir)
                
                # INCLUDE'ları kategorize et
                common_include_indices = set()  # COMMON INCLUDE satır indeksleri
                structure_include_count = 0
                common_include_count = 0
                
                for inc in all_includes:
                    abs_path_upper = inc['abs_path'].upper()
                    
                    # Bu INCLUDE COMMON LOAD/THERMAL mı?
                    is_common = False
                    if common_type == 'LOAD':
                        if '_COMMON_/LOAD' in abs_path_upper or '/COMMON/LOAD' in abs_path_upper:
                            is_common = True
                    elif common_type == 'THERMAL':
                        if '_COMMON_/THERMAL' in abs_path_upper or '/COMMON/THERMAL' in abs_path_upper:
                            is_common = True
                    
                    if is_common:
                        # COMMON INCLUDE - path'i kaydet, satırları atla
                        common_includes_raw.append(inc['abs_path'])
                        for idx in range(inc['start_idx'], inc['end_idx'] + 1):
                            common_include_indices.add(idx)
                        common_include_count += 1
                    else:
                        # Structure/Interface/vs INCLUDE - tek satır INCLUDE olarak ekle (pyNastran açacak)
                        # Absolute path ile yeni INCLUDE satırı oluştur
                        include_line = f"INCLUDE '{inc['abs_path']}'"
                        all_lines_raw.append(include_line)
                        # Orijinal satırları atla (common_include_indices'e ekle)
                        for idx in range(inc['start_idx'], inc['end_idx'] + 1):
                            common_include_indices.add(idx)
                        structure_include_count += 1
                
                # TÜM SATIRLARI oku (INCLUDE satırları hariç - hem COMMON hem diğerleri)
                lines = content.split('\n')
                line_count = 0
                
                for idx, line in enumerate(lines):
                    # INCLUDE satırı mı? Atla (zaten yukarıda işledik)
                    if idx in common_include_indices:
                        continue
                    
                    stripped = line.strip()
                    if not stripped:
                        continue
                    
                    # Case control satırlarını atla
                    upper = stripped.upper()
                    skip_keywords = ['SOL ', 'SOL\t', 'CEND', 'TITLE', 'SUBTITLE', 'ECHO', 
                                    'SUBCASE', 'LOAD =', 'LOAD=', 'SPC =', 'SPC=', 
                                    'TEMPERATURE', 'DISPLACEMENT', 'FORCE', 'GPFORCE', 
                                    'OLOAD', 'SPCFORCE', 'SET ', 'BEGIN BULK', 
                                    'BEGIN,BULK', 'ENDDATA']
                    is_skip = any(upper.startswith(kw) for kw in skip_keywords)
                    
                    if not is_skip:
                        all_lines_raw.append(line)  # Orijinal satırı koru
                        line_count += 1
                
                self.log1(f"        -> {line_count} data lines, {structure_include_count} structure includes, {common_include_count} common includes")
        
        self.log1(f"    Matched {matched_count} MASTER BDFs with {matched_subcases} total subcases")
        self.log1(f"    Total raw lines (including structure includes): {len(all_lines_raw)}")
        self.log1(f"    Total raw COMMON includes: {len(common_includes_raw)}")
        
        # Duplicate satırları çıkar
        self.log1("    Removing duplicate lines...")
        seen_lines = set()
        unique_lines = []
        for line in all_lines_raw:
            # Normalize et (boşlukları düzenle)
            normalized = ' '.join(line.split())
            if normalized not in seen_lines:
                seen_lines.add(normalized)
                unique_lines.append(line)
        
        # Duplicate COMMON INCLUDE'ları çıkar
        seen_includes = set()
        unique_common_includes = []
        for inc_path in common_includes_raw:
            normalized = inc_path.lower().replace('\\', '/')
            if normalized not in seen_includes:
                seen_includes.add(normalized)
                unique_common_includes.append(inc_path)
        
        self.log1(f"    After removing duplicates:")
        self.log1(f"      Unique lines: {len(unique_lines)} (removed {len(all_lines_raw) - len(unique_lines)} duplicates)")
        self.log1(f"      Unique COMMON includes: {len(unique_common_includes)}")
        
        return unique_lines, unique_common_includes, subcase_info_map
    
    def extract_param_cards(self, bulk_data):
        """
        Bulk data'dan PARAM kartlarını ayırır.
        Returns: (param_lines, remaining_bulk_data)
        """
        lines = bulk_data.split('\n')
        param_lines = []
        other_lines = []
        
        seen_params = set()  # Duplicate PARAM kontrolü
        
        i = 0
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()
            upper = stripped.upper()
            
            if upper.startswith('PARAM'):
                # PARAM kartı - continuation satırlarını da al
                param_block = [line]
                
                # PARAM name'ini çıkar (duplicate kontrolü için)
                param_name = None
                try:
                    if ',' in line:
                        parts = line.split(',')
                        if len(parts) > 1:
                            param_name = parts[1].strip().upper()
                    else:
                        if len(line) >= 16:
                            param_name = line[8:16].strip().upper()
                except:
                    pass
                
                i += 1
                # Continuation satırlarını kontrol et
                while i < len(lines):
                    next_line = lines[i]
                    next_stripped = next_line.strip()
                    is_cont = (next_line.startswith('+') or 
                              next_line.startswith('*') or
                              (next_line.startswith('        ') and next_stripped and 
                               not next_stripped.startswith('$') and
                               not any(next_stripped.upper().startswith(card) for card in 
                                      ['PARAM', 'GRID', 'CBAR', 'CBEAM', 'CQUAD', 'CTRIA', 
                                       'MAT', 'PBAR', 'PSHELL', 'FORCE', 'MOMENT', 'RBE',
                                       'CORD', 'SPC', 'MPC', 'INCLUDE', 'ENDDATA'])))
                    if is_cont and next_stripped:
                        param_block.append(next_line)
                        i += 1
                    else:
                        break
                
                # Duplicate kontrolü
                if param_name and param_name not in seen_params:
                    seen_params.add(param_name)
                    param_lines.extend(param_block)
            else:
                other_lines.append(line)
                i += 1
        
        return param_lines, '\n'.join(other_lines)
    
    def check_and_remove_duplicates(self, bulk_data):
        """
        Bulk data içindeki duplicate kartları tespit edip kaldırır.
        
        - Element/Property/Material kartları: ID bazlı kontrol (aynı ID → duplicate)
        - SPC/FORCE/MOMENT kartları: Tüm satır bazlı kontrol (birebir aynıysa → duplicate)
        """
        self.log1("    Checking for duplicate entries...")
        
        lines = bulk_data.split('\n')
        
        # ID bazlı kontrol yapılacak kartlar
        id_based_cards = {
            'GRID': set(),
            'CBAR': set(),
            'CBEAM': set(),
            'CROD': set(),
            'CONROD': set(),
            'CQUAD4': set(),
            'CQUAD8': set(),
            'CTRIA3': set(),
            'CTRIA6': set(),
            'CHEXA': set(),
            'CPENTA': set(),
            'CTETRA': set(),
            'CBUSH': set(),
            'CELAS1': set(),
            'CELAS2': set(),
            'CDAMP1': set(),
            'CDAMP2': set(),
            'CMASS1': set(),
            'CMASS2': set(),
            'RBE2': set(),
            'RBE3': set(),
            'PBAR': set(),
            'PBARL': set(),
            'PBEAM': set(),
            'PBEAML': set(),
            'PROD': set(),
            'PSHELL': set(),
            'PCOMP': set(),
            'PCOMPG': set(),
            'PSOLID': set(),
            'PBUSH': set(),
            'PELAS': set(),
            'PDAMP': set(),
            'PMASS': set(),
            'PTUBE': set(),
            'PVISC': set(),
            'PGAP': set(),
            'PWELD': set(),
            'MAT1': set(),
            'MAT2': set(),
            'MAT8': set(),
            'MAT9': set(),
            'MATS1': set(),
            'CORD1R': set(),
            'CORD2R': set(),
            'CORD1C': set(),
            'CORD2C': set(),
            'CORD1S': set(),
            'CORD2S': set(),
        }
        
        # Tüm satır bazlı kontrol yapılacak kartlar (SPC, FORCE, MOMENT, MPC)
        line_based_cards = ['SPC', 'SPC1', 'FORCE', 'MOMENT', 'MPC', 'LOAD', 'TEMP', 'TEMPD']
        seen_full_lines = set()  # Tüm satır için
        
        # İstatistikler
        duplicate_counts = {}
        
        result_lines = []
        i = 0
        
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()
            
            # Boş veya comment satırı
            if not stripped or stripped.startswith('$'):
                result_lines.append(line)
                i += 1
                continue
            
            upper = stripped.upper()
            
            # Hangi tip kart?
            card_type = None
            is_line_based = False
            
            # Önce line-based kartları kontrol et
            for ctype in line_based_cards:
                if upper.startswith(ctype) and (len(upper) == len(ctype) or 
                    upper[len(ctype)] in ' ,\t*'):
                    card_type = ctype
                    is_line_based = True
                    break
            
            # Sonra ID-based kartları kontrol et
            if not card_type:
                for ctype in id_based_cards.keys():
                    if upper.startswith(ctype) and (len(upper) == len(ctype) or 
                        upper[len(ctype)] in ' ,\t*'):
                        card_type = ctype
                        break
            
            if card_type:
                if is_line_based:
                    # LINE-BASED: Tüm satırı (ve continuation'ları) karşılaştır
                    full_card = [line]
                    j = i + 1
                    
                    # Continuation satırlarını topla
                    while j < len(lines):
                        next_line = lines[j]
                        next_stripped = next_line.strip()
                        is_cont = (next_line.startswith('+') or 
                                  next_line.startswith('*') or
                                  (next_line.startswith('        ') and next_stripped and 
                                   not next_stripped.startswith('$') and
                                   not any(next_stripped.upper().startswith(ct) for ct in 
                                          list(id_based_cards.keys()) + line_based_cards)))
                        if is_cont and next_stripped:
                            full_card.append(next_line)
                            j += 1
                        else:
                            break
                    
                    # Normalize edilmiş tam kart
                    normalized_card = '|'.join(' '.join(l.split()) for l in full_card)
                    
                    if normalized_card in seen_full_lines:
                        # Duplicate - atla
                        if card_type not in duplicate_counts:
                            duplicate_counts[card_type] = 0
                        duplicate_counts[card_type] += 1
                        i = j
                        continue
                    else:
                        seen_full_lines.add(normalized_card)
                        result_lines.extend(full_card)
                        i = j
                        continue
                else:
                    # ID-BASED: Sadece ID'ye bak
                    card_id = None
                    try:
                        if ',' in line:
                            parts = line.split(',')
                            if len(parts) > 1:
                                id_str = parts[1].strip()
                                if id_str:
                                    card_id = int(float(id_str))
                        else:
                            if len(line) >= 16:
                                id_str = line[8:16].strip()
                                if id_str:
                                    card_id = int(float(id_str))
                    except:
                        pass
                    
                    if card_id is not None:
                        if card_id in id_based_cards[card_type]:
                            # Duplicate - atla
                            if card_type not in duplicate_counts:
                                duplicate_counts[card_type] = 0
                            duplicate_counts[card_type] += 1
                            
                            # Continuation satırlarını da atla
                            i += 1
                            while i < len(lines):
                                next_line = lines[i]
                                next_stripped = next_line.strip()
                                is_cont = (next_line.startswith('+') or 
                                          next_line.startswith('*') or
                                          (next_line.startswith('        ') and next_stripped and 
                                           not next_stripped.startswith('$') and
                                           not any(next_stripped.upper().startswith(ct) for ct in 
                                                  list(id_based_cards.keys()) + line_based_cards)))
                                if is_cont and next_stripped:
                                    i += 1
                                else:
                                    break
                            continue
                        else:
                            id_based_cards[card_type].add(card_id)
            
            result_lines.append(line)
            i += 1
        
        # Rapor
        if duplicate_counts:
            self.log1("    Removed duplicates:")
            for ctype, count in sorted(duplicate_counts.items()):
                self.log1(f"      {ctype}: {count}")
            total = sum(duplicate_counts.values())
            self.log1(f"      TOTAL: {total} duplicate entries removed")
        else:
            self.log1("    No duplicates found")
        
        return '\n'.join(result_lines)
    
    def merge_lines_with_pynastran(self, lines):
        """
        Satırları temp BDF'e yazıp pyNastran ile merge eder.
        Duplicate ID hatası olursa, satırları direkt yazar.
        Returns: merged bulk data string
        """
        if not lines:
            self.log1("    WARNING: No lines to merge!")
            return ""
        
        temp_bdf_path = os.path.join(tempfile.gettempdir(), "_temp_lines_to_merge.bdf")
        
        self.log1(f"    Writing {len(lines)} lines to temp BDF...")
        
        try:
            # Temp BDF oluştur
            with open(temp_bdf_path, 'w', encoding='utf-8') as f:
                f.write("$ Temporary BDF for merging\n")
                f.write("SOL 101\n")
                f.write("CEND\n")
                f.write("BEGIN BULK\n")
                for line in lines:
                    f.write(line + "\n")
                f.write("ENDDATA\n")
            
            self.log1(f"    Reading temp BDF with pyNastran (following includes)...")
            
            try:
                bdf = BDF(debug=False)
                # allow_duplicate_ids ile dene
                bdf.read_bdf(temp_bdf_path, validate=False, xref=False, 
                            read_includes=True, save_file_structure=False)
                
                self.log1(f"      Loaded: {len(bdf.nodes)} nodes, {len(bdf.elements)} elements")
                self.log1(f"      Properties: {len(bdf.properties)}, Materials: {len(bdf.materials)}")
                self.log1(f"      Coords: {len(bdf.coords)}, MPCs: {len(bdf.mpcs)}, SPCs: {len(bdf.spcs)}")
                
                # Merge edilmiş BDF'i yaz
                merged_temp_path = os.path.join(tempfile.gettempdir(), "_temp_merged.bdf")
                bdf.write_bdf(merged_temp_path, size=8, is_double=False)
                
                with open(merged_temp_path, 'r', errors='ignore') as f:
                    merged_content = f.read()
                
                if os.path.exists(merged_temp_path): os.remove(merged_temp_path)
                
            except Exception as e:
                self.log1(f"    pyNastran failed: {str(e)[:100]}")
                self.log1(f"    Falling back to direct file reading...")
                
                # pyNastran başarısız oldu - INCLUDE'ları manuel aç
                merged_content = self.expand_includes_manually(temp_bdf_path)
            
            # Temizlik
            if os.path.exists(temp_bdf_path): os.remove(temp_bdf_path)
            
            # BEGIN BULK sonrasını al
            bulk_match = re.search(r'BEGIN\s*,?\s*BULK', merged_content, re.IGNORECASE)
            if bulk_match:
                bulk_data = merged_content[bulk_match.end():]
            else:
                bulk_data = merged_content
            
            # Gereksiz satırları temizle
            result_lines = []
            for l in bulk_data.split('\n'):
                if l.startswith('$pyNastran'): continue
                if l.strip().upper().startswith('ENDDATA'): continue
                if l.strip().upper().startswith('INCLUDE'): continue
                if l.strip().upper().startswith('SOL '): continue
                if l.strip().upper().startswith('CEND'): continue
                if l.strip().upper().startswith('BEGIN'): continue
                result_lines.append(l)
            
            result = '\n'.join(result_lines)
            self.log1(f"      Merged bulk data: {len(result)} characters")
            return result
            
        except Exception as e:
            self.log1(f"    ERROR merging: {e}")
            import traceback
            self.log1(traceback.format_exc())
            if os.path.exists(temp_bdf_path): os.remove(temp_bdf_path)
            return ""
    
    def expand_includes_manually(self, bdf_path):
        """
        INCLUDE'ları manuel olarak açar (pyNastran başarısız olduğunda).
        """
        self.log1("    Expanding includes manually...")
        
        content = self.read_file_safe(bdf_path)
        bdf_dir = os.path.dirname(os.path.abspath(bdf_path))
        
        # INCLUDE'ları bul ve aç
        all_includes = self.parse_multiline_includes(content, bdf_dir)
        
        # Satırları işle
        lines = content.split('\n')
        result_lines = []
        
        # INCLUDE satır indekslerini topla
        include_indices = {}
        for inc in all_includes:
            for idx in range(inc['start_idx'], inc['end_idx'] + 1):
                include_indices[idx] = inc
        
        processed_includes = set()
        
        for idx, line in enumerate(lines):
            if idx in include_indices:
                inc = include_indices[idx]
                # Sadece start_idx'te işle (continuation satırlarını atla)
                if idx == inc['start_idx']:
                    inc_path = inc['abs_path']
                    if inc_path not in processed_includes:
                        processed_includes.add(inc_path)
                        # INCLUDE dosyasını oku
                        if os.path.exists(inc_path):
                            try:
                                inc_content = self.read_file_safe(inc_path)
                                result_lines.append(f"$ === EXPANDED: {os.path.basename(inc_path)} ===")
                                for inc_line in inc_content.split('\n'):
                                    # Recursive INCLUDE'ları atla (basit tutuyoruz)
                                    if not inc_line.strip().upper().startswith('INCLUDE'):
                                        result_lines.append(inc_line)
                            except Exception as e:
                                result_lines.append(f"$ ERROR reading {inc_path}: {e}")
                        else:
                            result_lines.append(f"$ FILE NOT FOUND: {inc_path}")
            else:
                result_lines.append(line)
        
        self.log1(f"      Expanded {len(processed_includes)} includes")
        return '\n'.join(result_lines)
    
    def start_processing(self):
        if not self.thermal_bdfs and not self.maneuver_bdfs:
            messagebox.showerror("Error","Add BDF files"); return
        if not self.excel_path.get():
            messagebox.showerror("Error","Select Excel"); return
        if not self.output_folder.get():
            messagebox.showerror("Error","Select output folder"); return
        self.process_btn.config(state=tk.DISABLED)
        self.progress1.start()
        threading.Thread(target=self.process_merge, daemon=True).start()
    
    def process_merge(self):
        try:
            self.log1("="*70)
            self.log1("BDF Merger Tool v8")
            self.log1("="*70)
            
            self.log1("\n[1] Reading Excel...")
            xl = pd.ExcelFile(self.excel_path.get())
            sheets = xl.sheet_names
            self.log1(f"    Available sheets: {sheets}")

            thermal_sh = maneuver_sh = element_sh = None

            # First pass: look for exact or specific matches
            for s in sheets:
                sl = s.lower()
                # Element_Set exact match (priority)
                if sl == 'element_set' or sl == 'elementset':
                    element_sh = s
                # Thermal
                elif 'thermal' in sl and not thermal_sh:
                    thermal_sh = s
                # Maneuver
                elif ('maneuver' in sl or 'manevra' in sl) and not maneuver_sh:
                    maneuver_sh = s

            # Second pass: if element_sh not found, look for partial matches
            if not element_sh:
                for s in sheets:
                    sl = s.lower()
                    if 'element' in sl and 'set' in sl:
                        element_sh = s
                        break

            # Fallback to index-based if still not found
            if not thermal_sh and len(sheets) > 0: thermal_sh = sheets[0]
            if not maneuver_sh and len(sheets) > 1: maneuver_sh = sheets[1]
            if not element_sh and len(sheets) > 2: element_sh = sheets[2]

            self.log1(f"    Using sheets -> Thermal: '{thermal_sh}', Maneuver: '{maneuver_sh}', Element_Set: '{element_sh}'")
            
            thermal_cases = pd.read_excel(xl, sheet_name=thermal_sh).iloc[:,0].dropna().astype(int).tolist() if thermal_sh else []
            maneuver_cases = pd.read_excel(xl, sheet_name=maneuver_sh).iloc[:,0].dropna().astype(int).tolist() if maneuver_sh else []
            element_ids = sorted(pd.read_excel(xl, sheet_name=element_sh).iloc[:,0].dropna().astype(int).tolist()) if element_sh else []
            
            self.log1(f"    Thermal cases: {len(thermal_cases)}")
            self.log1(f"    Maneuver cases: {len(maneuver_cases)}")
            self.log1(f"    Element IDs: {len(element_ids)}")
            
            set_id = int(self.set_id.get())
            temp_initial = self.temp_initial.get()
            out_dir = self.output_folder.get()
            os.makedirs(out_dir, exist_ok=True)
            
            if self.thermal_bdfs:
                self.log1("\n" + "="*70)
                self.log1("[2] Processing THERMAL...")
                self.log1("="*70)
                self.process_thermal_bdf(self.thermal_bdfs, thermal_cases, element_ids, set_id,
                    temp_initial, os.path.join(out_dir, self.output_thermal_name.get()))
            
            if self.maneuver_bdfs:
                self.log1("\n" + "="*70)
                self.log1("[3] Processing MANEUVER...")
                self.log1("="*70)
                self.process_maneuver_bdf(self.maneuver_bdfs, maneuver_cases, element_ids, set_id,
                    os.path.join(out_dir, self.output_maneuver_name.get()))
            
            self.log1("\n" + "="*70)
            self.log1("COMPLETED!")
            self.log1("="*70)
            self.root.after(0, lambda: messagebox.showinfo("Done","Merge completed!"))
        except Exception as e:
            self.log1(f"\nERROR: {e}")
            import traceback
            self.log1(traceback.format_exc())
            self.root.after(0, lambda: messagebox.showerror("Error",str(e)))
        finally:
            self.root.after(0, lambda: [self.progress1.stop(), self.process_btn.config(state=tk.NORMAL)])
    
    def process_thermal_bdf(self, bdf_files, load_cases, element_ids, set_id, temp_initial, output_path):
        self.log1(f"    MASTER BDFs: {len(bdf_files)}")
        self.log1(f"    Load cases to match: {len(load_cases)}")
        load_case_set = set(load_cases)
        
        # Step 1: Tüm satırları topla
        self.log1("\n    === Step 1: Collecting all lines from MASTER BDFs ===")
        all_lines, common_includes, subcase_info_map = self.collect_all_lines_from_masters(
            bdf_files, load_case_set, 'THERMAL'
        )
        
        # Step 2: pyNastran ile merge et
        self.log1("\n    === Step 2: Merging with pyNastran ===")
        merged_bulk_data = self.merge_lines_with_pynastran(all_lines)
        
        # Step 2.5: Duplicate kontrolü
        if merged_bulk_data:
            self.log1("\n    === Step 2.5: Checking duplicates ===")
            merged_bulk_data = self.check_and_remove_duplicates(merged_bulk_data)
        
        # Step 3: Output dosyası oluştur
        self.log1("\n    === Step 3: Writing output BDF ===")
        out = []
        out.append(f"$ {'='*60}")
        out.append(f"$ THERMAL - MERGED BDF (v8)")
        out.append(f"$ {'='*60}")
        out.append("SOL 101")
        out.append("CEND")
        out.append("ECHO=NONE")
        out.append(f"TITLE = THERMAL ANALYSIS")
        out.append(f"TEMPERATURE(INITIAL) = {temp_initial}")
        out.append("$")
        
        # SET definition
        chunks = []
        current = ""
        for eid in element_ids:
            test = f"{current},{eid}" if current else str(eid)
            if len(test) > 60 and current:
                chunks.append(current)
                current = str(eid)
            else:
                current = test
        if current: chunks.append(current)
        
        for i, chunk in enumerate(chunks):
            if i == 0:
                out.append(f"SET {set_id} = {chunk}" + ("," if len(chunks) > 1 else ""))
            elif i == len(chunks) - 1:
                out.append(f"         {chunk}")
            else:
                out.append(f"         {chunk},")
        
        out.append("$")
        out.append("DISPLACEMENT(SORT1,PLOT,REAL)=ALL")
        out.append(f"FORCE(SORT1,PLOT,REAL,CENTER)={set_id}")
        out.append("GPFORCE(PLOT)=ALL")
        out.append("OLOAD(PLOT)=ALL")
        out.append("SPCFORCE(SORT1,PLOT)=ALL")
        out.append("$")
        
        # SUBCASE definitions
        for lc in load_cases:
            if lc in subcase_info_map:
                info = subcase_info_map[lc]
                temp_load = info['temp_load_id']
                subtitle = info['subtitle']
            else:
                temp_load = lc
                subtitle = f"Thermal Case {lc}"
            out.append(f"SUBCASE {lc}")
            out.append(f"SUBTITLE {subtitle}")
            out.append("SPC = 1")
            out.append(f"TEMPERATURE(LOAD) = {temp_load}")
            out.append("$")
        
        out.append("BEGIN BULK")
        
        # PARAM kartlarını ayır ve BEGIN BULK'tan hemen sonra yaz
        param_lines = []
        if merged_bulk_data:
            param_lines, merged_bulk_data = self.extract_param_cards(merged_bulk_data)
        
        if param_lines:
            out.append("$ --- PARAM CARDS ---")
            out.extend(param_lines)
            out.append("$")
        
        # Merged bulk data
        if merged_bulk_data:
            out.append(f"$ {'='*60}")
            out.append(f"$ MERGED STRUCTURE DATA")
            out.append(f"$ {'='*60}")
            out.append(merged_bulk_data)
        
        # Common Thermal INCLUDE'ları
        out.append("$")
        out.append(f"$ {'='*60}")
        out.append(f"$ COMMON THERMAL INCLUDES ({len(common_includes)} files)")
        out.append(f"$ {'='*60}")
        
        for abs_path in sorted(common_includes):
            include_lines = self.format_include_nastran(abs_path)
            out.extend(include_lines)
        
        out.append("$")
        out.append("ENDDATA")
        
        with open(output_path, 'w') as f:
            f.write('\n'.join(out))
        
        self.log1(f"    Output: {os.path.basename(output_path)}")
        self.log1(f"    COMMON THERMAL INCLUDES: {len(common_includes)}")
    
    def process_maneuver_bdf(self, bdf_files, load_cases, element_ids, set_id, output_path):
        self.log1(f"    MASTER BDFs: {len(bdf_files)}")
        self.log1(f"    Load cases to match: {len(load_cases)}")
        load_case_set = set(load_cases)
        
        # Step 1: Tüm satırları topla
        self.log1("\n    === Step 1: Collecting all lines from MASTER BDFs ===")
        all_lines, common_includes, subcase_info_map = self.collect_all_lines_from_masters(
            bdf_files, load_case_set, 'LOAD'
        )
        
        # Step 2: pyNastran ile merge et
        self.log1("\n    === Step 2: Merging with pyNastran ===")
        merged_bulk_data = self.merge_lines_with_pynastran(all_lines)
        
        # Step 2.5: Duplicate kontrolü
        if merged_bulk_data:
            self.log1("\n    === Step 2.5: Checking duplicates ===")
            merged_bulk_data = self.check_and_remove_duplicates(merged_bulk_data)
        
        # Step 3: Output dosyası oluştur
        self.log1("\n    === Step 3: Writing output BDF ===")
        out = []
        out.append(f"$ {'='*60}")
        out.append(f"$ MANEUVER - MERGED BDF (v8)")
        out.append(f"$ {'='*60}")
        out.append("SOL 101")
        out.append("CEND")
        out.append("ECHO=NONE")
        out.append(f"TITLE = MANEUVER ANALYSIS")
        out.append("$")
        
        # SET definition
        chunks = []
        current = ""
        for eid in element_ids:
            test = f"{current},{eid}" if current else str(eid)
            if len(test) > 60 and current:
                chunks.append(current)
                current = str(eid)
            else:
                current = test
        if current: chunks.append(current)
        
        for i, chunk in enumerate(chunks):
            if i == 0:
                out.append(f"SET {set_id} = {chunk}" + ("," if len(chunks) > 1 else ""))
            elif i == len(chunks) - 1:
                out.append(f"         {chunk}")
            else:
                out.append(f"         {chunk},")
        
        out.append("$")
        out.append("DISPLACEMENT(SORT1,PLOT,REAL)=ALL")
        out.append(f"FORCE(SORT1,PLOT,REAL,CENTER)={set_id}")
        out.append("GPFORCE(PLOT)=ALL")
        out.append("OLOAD(PLOT)=ALL")
        out.append("SPCFORCE(SORT1,PLOT)=ALL")
        out.append("$")
        
        # SUBCASE definitions
        for lc in load_cases:
            if lc in subcase_info_map:
                info = subcase_info_map[lc]
                load_id = info['load_id']
                subtitle = info['subtitle']
            else:
                load_id = lc
                subtitle = f"Manoeuvre {lc}"
            out.append(f"SUBCASE {lc}")
            out.append(f"SUBTITLE {subtitle}")
            out.append("SPC = 1")
            out.append(f"LOAD = {load_id}")
            out.append("$")
        
        out.append("BEGIN BULK")
        
        # PARAM kartlarını ayır ve BEGIN BULK'tan hemen sonra yaz
        param_lines = []
        if merged_bulk_data:
            param_lines, merged_bulk_data = self.extract_param_cards(merged_bulk_data)
        
        if param_lines:
            out.append("$ --- PARAM CARDS ---")
            out.extend(param_lines)
            out.append("$")
        
        # Merged bulk data
        if merged_bulk_data:
            out.append(f"$ {'='*60}")
            out.append(f"$ MERGED STRUCTURE DATA")
            out.append(f"$ {'='*60}")
            out.append(merged_bulk_data)
        
        # Common Load INCLUDE'ları
        out.append("$")
        out.append(f"$ {'='*60}")
        out.append(f"$ COMMON LOAD INCLUDES ({len(common_includes)} files)")
        out.append(f"$ {'='*60}")
        
        for abs_path in sorted(common_includes):
            include_lines = self.format_include_nastran(abs_path)
            out.extend(include_lines)
        
        out.append("$")
        out.append("ENDDATA")
        
        with open(output_path, 'w') as f:
            f.write('\n'.join(out))
        
        self.log1(f"    Output: {os.path.basename(output_path)}")
        self.log1(f"    COMMON LOAD INCLUDES: {len(common_includes)}")

    # ============= TAB 2 HELPERS =============
    def add_run_bdfs(self):
        files = filedialog.askopenfilenames(filetypes=[("BDF","*.bdf *.dat *.nas"),("All","*.*")])
        for f in files:
            if f not in self.run_bdfs:
                self.run_bdfs.append(f)
                self.run_listbox.insert(tk.END, f)
        self.run_count.set(f"{len(self.run_bdfs)} files")
    
    def clear_run_bdfs(self):
        self.run_bdfs.clear()
        self.run_listbox.delete(0, tk.END)
        self.run_count.set("0 files")
    
    def browse_property_excel(self):
        f = filedialog.askopenfilename(filetypes=[("Excel","*.xlsx *.xls")])
        if f: self.property_excel_path.set(f)
    
    def browse_nastran(self):
        f = filedialog.askopenfilename(filetypes=[("All","*.*")])
        if f: self.nastran_path.set(f)
    
    def browse_run_output(self):
        f = filedialog.askdirectory()
        if f: self.run_output_folder.set(f)
    
    def log2(self, msg):
        self.log_text2.insert(tk.END, msg + "\n")
        self.log_text2.see(tk.END)
        self.root.update_idletasks()
    
    def clear_log2(self):
        self.log_text2.delete(1.0, tk.END)
    
    def load_properties(self):
        if not self.property_excel_path.get():
            messagebox.showerror("Error","Select Excel"); return
        try:
            xl = pd.ExcelFile(self.property_excel_path.get())
            sheets = xl.sheet_names
            bar_sh = skin_sh = res_sh = None

            # First pass: exact matches (priority)
            for s in sheets:
                sl = s.lower().replace('_',' ').replace('-',' ')
                if sl == 'skin property' or sl == 'skinproperty':
                    skin_sh = s
                elif sl == 'bar property' or sl == 'barproperty':
                    bar_sh = s
                elif sl == 'residual strength' or sl == 'residualstrength':
                    res_sh = s

            # Second pass: partial matches
            for s in sheets:
                sl = s.lower().replace('_',' ')
                if not bar_sh and 'bar' in sl and 'prop' in sl:
                    bar_sh = s
                elif not skin_sh and 'skin' in sl and 'prop' in sl:
                    skin_sh = s
                elif not res_sh and ('residual' in sl or 'strength' in sl):
                    res_sh = s

            self.bar_properties.clear()
            self.skin_properties.clear()

            if bar_sh:
                df = pd.read_excel(xl, sheet_name=bar_sh)
                for _, row in df.iterrows():
                    try:
                        pid = int(row.iloc[0])
                        d1 = float(row.iloc[1]) if len(df.columns)>1 else 0
                        d2 = float(row.iloc[2]) if len(df.columns)>2 else 0
                        self.bar_properties[pid] = {'dim1':d1, 'dim2':d2}
                    except: pass

            if skin_sh:
                df = pd.read_excel(xl, sheet_name=skin_sh)
                for _, row in df.iterrows():
                    try:
                        pid = int(row.iloc[0])
                        t = float(row.iloc[1])
                        self.skin_properties[pid] = {'thickness':t}
                    except: pass

            if res_sh:
                self.residual_strength_df = pd.read_excel(xl, sheet_name=res_sh)

            self.bar_prop_text.delete(1.0, tk.END)
            self.bar_prop_text.insert(tk.END, f"Bar: {len(self.bar_properties)} loaded")
            self.skin_prop_text.delete(1.0, tk.END)
            self.skin_prop_text.insert(tk.END, f"Skin: {len(self.skin_properties)} loaded")
            self.resid_text.delete(1.0, tk.END)
            self.resid_text.insert(tk.END, f"Residual: {'Yes' if self.residual_strength_df is not None else 'No'}")

            # Log which sheets were used
            print(f"[Load Properties] Bar sheet: {bar_sh}, Skin sheet: {skin_sh}")
            print(f"[Load Properties] Bar PIDs: {len(self.bar_properties)}, Skin PIDs: {len(self.skin_properties)}")
            if self.skin_properties:
                sample_pids = list(self.skin_properties.keys())[:5]
                print(f"[Load Properties] Sample Skin PIDs: {sample_pids}")

            messagebox.showinfo("OK", f"Bar:{len(self.bar_properties)} Skin:{len(self.skin_properties)}\n\nSheets used:\nBar: {bar_sh}\nSkin: {skin_sh}")
        except Exception as e:
            import traceback
            traceback.print_exc()
            messagebox.showerror("Error", str(e))
    
    def read_file(self, path):
        for enc in ['utf-8','latin-1','cp1252']:
            try:
                with open(path, 'r', encoding=enc) as f:
                    return f.read()
            except: pass
        return ""
    
    def copy_bdf_to_output(self, bdf_path, output_folder):
        bdf_name = os.path.basename(bdf_path)
        out_bdf = os.path.join(output_folder, bdf_name)
        shutil.copy2(bdf_path, out_bdf)
        return out_bdf
    
    def count_pcomp_plies(self, lines, start_idx):
        ply_count = 0
        j = start_idx + 1
        while j < len(lines):
            line = lines[j]
            stripped = line.strip()
            is_continuation = (
                line.startswith('+') or line.startswith('*') or
                (line.startswith(' ') and stripped and not stripped.startswith('$') and
                 not any(stripped.upper().startswith(card) for card in 
                        ['PSHELL','PCOMP','PBAR','PBARL','CBAR','CQUAD','CTRIA',
                         'GRID','MAT','FORCE','INCLUDE','END','SOL','CEND','BEGIN']))
            )
            if not is_continuation:
                break
            ply_count += 1
            j += 1
        return ply_count, j
    
    def update_properties_in_file(self, filepath):
        content = self.read_file(filepath)
        lines = content.split('\n')
        new_lines = []
        i = 0
        stats = {'pbarl': 0, 'pbar': 0, 'pshell': 0, 'pcomp': 0}
        warnings = []
        pshell_found = []
        pcomp_found = []
        
        while i < len(lines):
            line = lines[i]
            upper = line.upper().strip()
            
            if upper.startswith('PBARL'):
                try:
                    if ',' in line:
                        pid = int(float(line.split(',')[1].strip()))
                    else:
                        pid = int(float(line[8:16].strip()))
                    if i+1 < len(lines) and pid in self.bar_properties:
                        d1 = self.bar_properties[pid]['dim1']
                        d2 = self.bar_properties[pid]['dim2']
                        new_lines.append(line)
                        next_line = lines[i+1]
                        if ',' in next_line:
                            parts = next_line.split(',')
                            start_idx = 1 if parts[0].strip().startswith('+') else 0
                            if len(parts) > start_idx: parts[start_idx] = f"{d1}."
                            if len(parts) > start_idx + 1: parts[start_idx + 1] = f"{d2}."
                            new_lines.append(','.join(parts))
                        else:
                            cont = next_line[:8]
                            rest = next_line[24:] if len(next_line) > 24 else ""
                            d1_str = f"{d1:<8.6g}".rstrip()
                            if '.' not in d1_str and 'E' not in d1_str.upper(): d1_str += '.'
                            d2_str = f"{d2:<8.6g}".rstrip()
                            if '.' not in d2_str and 'E' not in d2_str.upper(): d2_str += '.'
                            new_lines.append(f"{cont}{d1_str:>8}{d2_str:>8}{rest}")
                        stats['pbarl'] += 1
                        i += 2
                        continue
                except: pass
                new_lines.append(line)
                i += 1
            
            elif upper.startswith('PBAR') and not upper.startswith('PBARL'):
                try:
                    if ',' in line:
                        pid = int(float(line.split(',')[1].strip()))
                    else:
                        pid = int(float(line[8:16].strip()))
                    if pid in self.bar_properties:
                        d1 = self.bar_properties[pid]['dim1']
                        d2 = self.bar_properties[pid]['dim2']
                        area = d1 * d2
                        if ',' in line:
                            parts = line.split(',')
                            parts[3] = str(area)
                            new_lines.append(','.join(parts))
                        else:
                            new_lines.append(line[:24] + f"{area:8.4g}" + line[32:])
                        stats['pbar'] += 1
                        i += 1
                        continue
                except: pass
                new_lines.append(line)
                i += 1
            
            elif upper.startswith('PSHELL'):
                try:
                    if ',' in line:
                        pid = int(float(line.split(',')[1].strip()))
                    else:
                        pid = int(float(line[8:16].strip()))
                    pshell_found.append(pid)
                    if pid in self.skin_properties:
                        t = self.skin_properties[pid]['thickness']
                        if ',' in line:
                            parts = line.split(',')
                            t_str = f"{t}"
                            if '.' not in t_str and 'E' not in t_str.upper():
                                t_str += '.'
                            parts[3] = t_str
                            new_lines.append(','.join(parts))
                        else:
                            t_str = f"{t:<8.6g}".rstrip()
                            if '.' not in t_str and 'E' not in t_str.upper():
                                t_str += '.'
                            new_lines.append(line[:24] + f"{t_str:>8}" + line[32:])
                        stats['pshell'] += 1
                        i += 1
                        continue
                except: pass
                new_lines.append(line)
                i += 1
            
            elif upper.startswith('PCOMP'):
                try:
                    if ',' in line:
                        pid = int(float(line.split(',')[1].strip()))
                    else:
                        pid = int(float(line[8:16].strip()))
                    pcomp_found.append(pid)
                    ply_count, end_idx = self.count_pcomp_plies(lines, i)
                    if pid in self.skin_properties:
                        if ply_count > 1:
                            warnings.append(f"PCOMP {pid}: {ply_count} plies - SKIPPED")
                            for k in range(i, end_idx):
                                new_lines.append(lines[k])
                            i = end_idx
                            continue
                        else:
                            t = self.skin_properties[pid]['thickness']
                            new_lines.append(line)
                            if i + 1 < end_idx:
                                cont_line = lines[i + 1]
                                if ',' in cont_line:
                                    parts = cont_line.split(',')
                                    if len(parts) >= 3:
                                        t_str = f"{t}"
                                        if '.' not in t_str and 'E' not in t_str.upper():
                                            t_str += '.'
                                        parts[2] = t_str
                                    new_lines.append(','.join(parts))
                                else:
                                    cont = cont_line[:8]
                                    mid = cont_line[8:16] if len(cont_line) > 8 else "        "
                                    rest = cont_line[24:] if len(cont_line) > 24 else ""
                                    t_str = f"{t:<8.6g}".rstrip()
                                    if '.' not in t_str and 'E' not in t_str.upper():
                                        t_str += '.'
                                    new_lines.append(f"{cont}{mid}{t_str:>8}{rest}")
                                for k in range(i + 2, end_idx):
                                    new_lines.append(lines[k])
                            stats['pcomp'] += 1
                            i = end_idx
                            continue
                    else:
                        for k in range(i, end_idx):
                            new_lines.append(lines[k])
                        i = end_idx
                        continue
                except: pass
                new_lines.append(line)
                i += 1
            else:
                new_lines.append(line)
                i += 1
        
        # Debug logging
        print(f"[Update Props] File: {os.path.basename(filepath)}")
        print(f"[Update Props] PSHELL found in BDF: {len(pshell_found)}, PIDs: {pshell_found[:10]}...")
        print(f"[Update Props] PCOMP found in BDF: {len(pcomp_found)}, PIDs: {pcomp_found[:10]}...")
        print(f"[Update Props] Skin properties loaded: {len(self.skin_properties)}")
        if pshell_found:
            matched = [p for p in pshell_found if p in self.skin_properties]
            not_matched = [p for p in pshell_found if p not in self.skin_properties]
            print(f"[Update Props] PSHELL matched: {len(matched)}, not matched: {len(not_matched)}")
            if not_matched:
                print(f"[Update Props] Sample unmatched PSHELL PIDs: {not_matched[:5]}")

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(new_lines))
        return stats, warnings
    
    def start_update_properties(self):
        if not self.run_bdfs:
            messagebox.showerror("Error","Add BDF files"); return
        if not self.bar_properties and not self.skin_properties:
            messagebox.showerror("Error","Load properties first"); return
        if not self.run_output_folder.get():
            messagebox.showerror("Error","Select output folder"); return
        self.btn1.config(state=tk.DISABLED)
        self.progress2.start()
        threading.Thread(target=self.do_update_properties, daemon=True).start()
    
    def do_update_properties(self):
        try:
            self.log2("="*60)
            self.log2("STEP 1: Update Properties")
            self.log2("="*60)
            self.log2(f"  Loaded Bar properties: {len(self.bar_properties)}")
            self.log2(f"  Loaded Skin properties: {len(self.skin_properties)}")
            if self.skin_properties:
                sample_pids = list(self.skin_properties.keys())[:5]
                self.log2(f"  Sample Skin PIDs: {sample_pids}")
            out_folder = self.run_output_folder.get()
            os.makedirs(out_folder, exist_ok=True)
            all_warnings = []
            for bdf_path in self.run_bdfs:
                self.log2(f"\n  Processing: {os.path.basename(bdf_path)}")
                self.log2("    Copying BDF to output folder...")
                out_bdf = self.copy_bdf_to_output(bdf_path, out_folder)
                self.log2("    Updating properties...")
                stats, warnings = self.update_properties_in_file(out_bdf)
                self.log2(f"    Updated: PBARL={stats['pbarl']} PBAR={stats['pbar']} PSHELL={stats['pshell']} PCOMP={stats['pcomp']}")
                all_warnings.extend(warnings)
            if all_warnings:
                self.log2("\n  Warnings:")
                for w in all_warnings[:10]:
                    self.log2(f"    {w}")
            self.log2("\n" + "="*60)
            self.log2("COMPLETED!")
            self.root.after(0, lambda: messagebox.showinfo("Done","Properties updated!"))
        except Exception as e:
            self.log2(f"ERROR: {e}")
            import traceback
            self.log2(traceback.format_exc())
            self.root.after(0, lambda: messagebox.showerror("Error",str(e)))
        finally:
            self.root.after(0, lambda: [self.progress2.stop(), self.btn1.config(state=tk.NORMAL)])
    
    def start_run_nastran(self):
        if not self.nastran_path.get():
            messagebox.showerror("Error","Select Nastran path"); return
        if not self.run_output_folder.get():
            messagebox.showerror("Error","Select output folder"); return
        self.btn2.config(state=tk.DISABLED)
        self.progress2.start()
        threading.Thread(target=self.do_run_nastran, daemon=True).start()
    
    def do_run_nastran(self):
        try:
            self.log2("="*60)
            self.log2("STEP 2: Run Nastran")
            self.log2("="*60)
            out_folder = self.run_output_folder.get()
            nastran = self.nastran_path.get()
            bdfs = [os.path.join(out_folder, f) for f in os.listdir(out_folder) if f.lower().endswith(('.bdf','.dat','.nas'))]
            for bdf in bdfs:
                self.log2(f"\n  Running: {os.path.basename(bdf)}")
                cmd = f'"{nastran}" "{bdf}" out="{out_folder}" scratch=yes batch=no'
                os.popen(cmd)
            self.log2("\nJobs submitted!")
            self.root.after(0, lambda: messagebox.showinfo("Done","Nastran jobs submitted!"))
        except Exception as e:
            self.log2(f"ERROR: {e}")
        finally:
            self.root.after(0, lambda: [self.progress2.stop(), self.btn2.config(state=tk.NORMAL)])
    
    def start_postprocess(self):
        if not self.run_output_folder.get():
            messagebox.showerror("Error","Select output folder"); return
        self.btn3.config(state=tk.DISABLED)
        self.progress2.start()
        threading.Thread(target=self.do_postprocess, daemon=True).start()
    
    def do_postprocess(self):
        try:
            self.log2("="*60)
            self.log2("STEP 3: Post-Process OP2")
            self.log2("="*60)
            out_folder = self.run_output_folder.get()
            op2_files = [os.path.join(out_folder, f) for f in os.listdir(out_folder) if f.lower().endswith('.op2')]
            if not op2_files:
                self.log2("No OP2 files found!")
                return
            
            # Element -> Property mapping ve PBARL dimensions
            elem_prop = {}
            pbarl_dims = {}  # {pid: {'dim1': x, 'dim2': y, 'type': 'ROD'/'BAR'}}
            
            # BDF'lerden oku
            bdf_files_to_read = list(self.run_bdfs)
            # Output folder'daki BDF'leri de ekle
            for f in os.listdir(out_folder):
                if f.lower().endswith(('.bdf', '.dat', '.nas')):
                    bdf_files_to_read.append(os.path.join(out_folder, f))
            
            for bdf_path in bdf_files_to_read:
                try:
                    self.log2(f"  Reading BDF: {os.path.basename(bdf_path)}")
                    bdf = BDF(debug=False)
                    bdf.read_bdf(bdf_path, validate=False, xref=False, read_includes=True)
                    
                    # Element -> Property mapping
                    for eid, el in bdf.elements.items():
                        if hasattr(el, 'pid'):
                            elem_prop[eid] = el.pid
                    
                    # PBARL dimensions
                    for pid, prop in bdf.properties.items():
                        prop_type = prop.type
                        if prop_type == 'PBARL':
                            # PBARL formatı: dim listesi var
                            dims = prop.dim if hasattr(prop, 'dim') else []
                            bar_type = prop.bar_type if hasattr(prop, 'bar_type') else 'UNKNOWN'
                            
                            if len(dims) >= 2:
                                pbarl_dims[pid] = {
                                    'dim1': dims[0],
                                    'dim2': dims[1],
                                    'type': bar_type
                                }
                            elif len(dims) == 1:
                                # ROD gibi tek dimension
                                pbarl_dims[pid] = {
                                    'dim1': dims[0],
                                    'dim2': dims[0],
                                    'type': bar_type
                                }
                        elif prop_type == 'PBAR':
                            # PBAR: A (area) var
                            area = prop.A if hasattr(prop, 'A') else None
                            if area:
                                # Area'dan approximate dims (kare varsayımı)
                                import math
                                side = math.sqrt(area) if area > 0 else 0
                                pbarl_dims[pid] = {
                                    'dim1': side,
                                    'dim2': side,
                                    'type': 'PBAR',
                                    'area': area
                                }
                    
                    self.log2(f"    Elements: {len(elem_prop)}, PBARL/PBAR props: {len(pbarl_dims)}")
                except Exception as e:
                    self.log2(f"    Warning reading BDF: {e}")
            
            self.log2(f"\n  Total: {len(elem_prop)} elements, {len(pbarl_dims)} bar properties from BDF")
            
            results = []
            for op2_path in op2_files:
                self.log2(f"\n  Processing: {os.path.basename(op2_path)}")
                try:
                    op2 = OP2(debug=False)
                    op2.read_op2(op2_path)
                    if hasattr(op2, 'cbar_force') and op2.cbar_force:
                        for sc_id, force in op2.cbar_force.items():
                            for i, eid in enumerate(force.element):
                                axial = force.data[0,i,6] if len(force.data.shape)==3 else force.data[i,6]
                                pid = elem_prop.get(eid)
                                d1 = d2 = area = stress = None
                                
                                # Önce Excel'den yüklenen properties'e bak
                                if pid and pid in self.bar_properties:
                                    d1 = self.bar_properties[pid]['dim1']
                                    d2 = self.bar_properties[pid]['dim2']
                                    area = d1 * d2
                                    if area > 0: stress = axial / area
                                # Excel'de yoksa BDF'den okunan PBARL'a bak
                                elif pid and pid in pbarl_dims:
                                    prop_info = pbarl_dims[pid]
                                    d1 = prop_info['dim1']
                                    d2 = prop_info['dim2']
                                    if 'area' in prop_info:
                                        area = prop_info['area']
                                    else:
                                        area = d1 * d2
                                    if area > 0: stress = axial / area
                                
                                results.append({'OP2': os.path.basename(op2_path), 'Subcase': sc_id, 'Element': eid,
                                    'Property': pid, 'Axial': axial, 'Dim1': d1, 'Dim2': d2, 'Area': area, 'Stress': stress})
                except Exception as e:
                    self.log2(f"    ERROR: {e}")
            
            csv_path = os.path.join(out_folder, self.csv_output_name.get())
            with open(csv_path, 'w', newline='') as f:
                w = csv.DictWriter(f, fieldnames=['OP2','Subcase','Element','Property','Axial','Dim1','Dim2','Area','Stress'])
                w.writeheader()
                w.writerows(results)
            self.log2(f"\n  Saved: {csv_path} ({len(results)} rows)")
            self.root.after(0, lambda: messagebox.showinfo("Done","Results saved"))
        except Exception as e:
            self.log2(f"ERROR: {e}")
            import traceback
            self.log2(traceback.format_exc())
        finally:
            self.root.after(0, lambda: [self.progress2.stop(), self.btn3.config(state=tk.NORMAL)])
    
    def start_combine_stress(self):
        if not self.run_output_folder.get():
            messagebox.showerror("Error","Select output folder"); return
        if self.residual_strength_df is None:
            messagebox.showerror("Error","Load Residual Strength data"); return
        self.btn4.config(state=tk.DISABLED)
        self.progress2.start()
        threading.Thread(target=self.do_combine_stress, daemon=True).start()
    
    def do_combine_stress(self):
        try:
            self.log2("="*60)
            self.log2("STEP 4: Combine Stress")
            self.log2("="*60)
            out_folder = self.run_output_folder.get()
            stress_csv = os.path.join(out_folder, self.csv_output_name.get())
            if not os.path.exists(stress_csv):
                self.log2("Stress CSV not found")
                return
            
            stress_df = pd.read_csv(stress_csv)
            lookup = {}
            for _, row in stress_df.iterrows():
                key = (int(row['Subcase']), int(row['Element']))
                lookup[key] = row['Stress'] if pd.notna(row['Stress']) else 0
            elements = stress_df['Element'].unique()
            
            rs_df = self.residual_strength_df
            cols = rs_df.columns.tolist()
            comb_col = cols[0]
            
            comp_cols = []
            i = 1
            while i < len(cols) - 1:
                col_name = str(cols[i]).upper()
                next_col_name = str(cols[i+1]).upper()
                if ('CASE' in col_name or 'ID' in col_name) and 'MULT' in next_col_name:
                    comp_cols.append((cols[i], cols[i+1]))
                    i += 2
                else:
                    i += 1
            
            results = []
            for _, rs_row in rs_df.iterrows():
                comb_lc = rs_row[comb_col]
                if pd.isna(comb_lc): continue
                comb_lc = int(comb_lc)
                
                for eid in elements:
                    total_stress = 0.0
                    components = []
                    for case_col, mult_col in comp_cols:
                        case_id = rs_row[case_col]
                        multiplier = rs_row[mult_col]
                        if pd.isna(case_id) or pd.isna(multiplier): continue
                        case_id = int(case_id)
                        multiplier = float(multiplier)
                        key = (case_id, int(eid))
                        if key in lookup:
                            stress = lookup[key]
                            if stress is not None:
                                total_stress += stress * multiplier
                                components.append(f"{case_id}*{multiplier}")
                    if components:
                        results.append({'Combined_LC': comb_lc, 'Element': eid, 
                            'Combined_Stress': total_stress, 'Components': ' + '.join(components)})
            
            comb_csv = os.path.join(out_folder, self.combined_csv_name.get())
            with open(comb_csv, 'w', newline='') as f:
                w = csv.DictWriter(f, fieldnames=['Combined_LC','Element','Combined_Stress','Components'])
                w.writeheader()
                w.writerows(results)
            self.log2(f"\n  Saved: {comb_csv}")
            self.root.after(0, lambda: messagebox.showinfo("Done","Combined stress saved"))
        except Exception as e:
            self.log2(f"ERROR: {e}")
        finally:
            self.root.after(0, lambda: [self.progress2.stop(), self.btn4.config(state=tk.NORMAL)])
    
    # ============= TAB 2 - OFFSET APPLICATION =============
    def browse_tab2_offset_csv(self):
        f = filedialog.askopenfilename(
            title="Select Offset CSV File",
            filetypes=[("CSV Files", "*.csv"), ("All Files", "*.*")]
        )
        if f:
            self.tab2_offset_csv.set(f)
            self.log2(f"Selected offset CSV: {os.path.basename(f)}")
    
    def start_apply_offsets_tab2(self):
        """Apply offsets to BDF files in Tab 2"""
        if not self.run_bdfs:
            messagebox.showerror("Error", "Please add BDF files first")
            return
        if not self.tab2_offset_csv.get():
            messagebox.showerror("Error", "Please select offset CSV file")
            return
        
        self.btn_apply_offset_tab2.config(state=tk.DISABLED)
        self.progress2.start()
        threading.Thread(target=self.apply_offsets_tab2, daemon=True).start()
    
    def apply_offsets_tab2(self):
        """Apply offsets from CSV to all BDF files - TEXT BASED (preserves INCLUDEs)"""
        try:
            self.log2("\n" + "="*60)
            self.log2("APPLYING OFFSETS TO BDF FILES (Text-based)")
            self.log2("="*60)

            # Read offset CSV
            self.log2("\nReading offset CSV...")
            landing_offsets = {}  # {eid: zoffset}
            bar_offsets = {}      # {eid: (offset_x, offset_y, offset_z)}

            with open(self.tab2_offset_csv.get(), 'r') as f:
                reader = csv.reader(f)
                section = None

                for row in reader:
                    if not row or not row[0]:
                        continue

                    if 'LANDING OFFSETS' in row[0]:
                        section = 'landing'
                        next(reader)  # Skip header
                        continue
                    elif 'BAR OFFSETS' in row[0]:
                        section = 'bar'
                        next(reader)  # Skip header
                        continue

                    if section == 'landing':
                        try:
                            eid = int(row[0])
                            zoffset = float(row[5])
                            landing_offsets[eid] = zoffset
                        except:
                            pass
                    elif section == 'bar':
                        try:
                            eid = int(row[0])
                            offset_x = float(row[8])
                            offset_y = float(row[9])
                            offset_z = float(row[10])
                            bar_offsets[eid] = (offset_x, offset_y, offset_z)
                        except:
                            pass

            self.log2(f"  Loaded {len(landing_offsets)} landing offsets")
            self.log2(f"  Loaded {len(bar_offsets)} bar offsets")

            # Helper function to format field (8-char fixed width)
            def fmt_field(value, width=8):
                if isinstance(value, float):
                    s = f"{value:.4f}"
                    if len(s) > width:
                        s = f"{value:.2E}"
                    return s[:width].ljust(width)
                return str(value)[:width].ljust(width)

            # Apply to each BDF file using text replacement
            total_landing = 0
            total_bar = 0

            for bdf_path in self.run_bdfs:
                self.log2(f"\n  Processing: {os.path.basename(bdf_path)}")

                # Read file as text
                with open(bdf_path, 'r', encoding='latin-1') as f:
                    lines = f.readlines()

                new_lines = []
                i = 0
                landing_modified = 0
                bar_modified = 0

                while i < len(lines):
                    line = lines[i]

                    # Check for CQUAD4 (landing/skin elements with zoffset)
                    if line.startswith('CQUAD4'):
                        # Parse element ID from field 2 (chars 8-16)
                        try:
                            eid = int(line[8:16].strip())
                            if eid in landing_offsets:
                                zoff = landing_offsets[eid]
                                # CQUAD4 format: CQUAD4, EID, PID, G1, G2, G3, G4, THETA, ZOFFS
                                # Fields are 8 chars each
                                # Field 8 (chars 56-64) is THETA/MCID, Field 9 (chars 64-72) is ZOFFS
                                if len(line) >= 64:
                                    # Modify ZOFFS field (field 9, chars 64-72)
                                    new_line = line[:64] + fmt_field(zoff) + line[72:] if len(line) > 72 else line[:64] + fmt_field(zoff) + '\n'
                                    new_lines.append(new_line)
                                    landing_modified += 1
                                    i += 1
                                    continue
                        except:
                            pass
                        new_lines.append(line)
                        i += 1
                        continue

                    # Check for CBAR elements
                    elif line.startswith('CBAR'):
                        # Parse element ID from field 2 (chars 8-16)
                        try:
                            eid = int(line[8:16].strip())
                            if eid in bar_offsets:
                                offset_vec = bar_offsets[eid]
                                # CBAR can be single line or multi-line (with continuation)
                                # Format: CBAR, EID, PID, GA, GB, X1, X2, X3, OFFT (line 1)
                                #         +, PA, PB, W1A, W2A, W3A, W1B, W2B, W3B (line 2)

                                # Check if next line is continuation
                                if i + 1 < len(lines) and (lines[i+1].startswith('+') or lines[i+1].startswith('*') or lines[i+1].startswith(' ')):
                                    # Multi-line CBAR - modify continuation line
                                    cont_line = lines[i+1]
                                    # Fields on continuation: +, PA, PB, W1A, W2A, W3A, W1B, W2B, W3B
                                    # W1A at field 4 (chars 24-32), W2A at field 5 (chars 32-40), W3A at field 6 (chars 40-48)
                                    # W1B at field 7 (chars 48-56), W2B at field 8 (chars 56-64), W3B at field 9 (chars 64-72)

                                    # Keep first part (continuation marker, PA, PB)
                                    new_cont = cont_line[:24]
                                    # Add W1A, W2A, W3A
                                    new_cont += fmt_field(offset_vec[0])
                                    new_cont += fmt_field(offset_vec[1])
                                    new_cont += fmt_field(offset_vec[2])
                                    # Add W1B, W2B, W3B (same as WA)
                                    new_cont += fmt_field(offset_vec[0])
                                    new_cont += fmt_field(offset_vec[1])
                                    new_cont += fmt_field(offset_vec[2])
                                    new_cont += '\n'

                                    new_lines.append(line)
                                    new_lines.append(new_cont)
                                    bar_modified += 1
                                    i += 2
                                    continue
                                else:
                                    # Single line CBAR - need to add continuation for offsets
                                    new_lines.append(line.rstrip() + '+CB' + str(eid)[-4:] + '\n')
                                    # Create continuation line with offsets
                                    cont_name = '+CB' + str(eid)[-4:]
                                    new_cont = cont_name.ljust(8)
                                    new_cont += '        '  # PA (blank)
                                    new_cont += '        '  # PB (blank)
                                    new_cont += fmt_field(offset_vec[0])  # W1A
                                    new_cont += fmt_field(offset_vec[1])  # W2A
                                    new_cont += fmt_field(offset_vec[2])  # W3A
                                    new_cont += fmt_field(offset_vec[0])  # W1B
                                    new_cont += fmt_field(offset_vec[1])  # W2B
                                    new_cont += fmt_field(offset_vec[2])  # W3B
                                    new_cont += '\n'
                                    new_lines.append(new_cont)
                                    bar_modified += 1
                                    i += 1
                                    continue
                        except:
                            pass
                        new_lines.append(line)
                        i += 1
                        continue

                    else:
                        new_lines.append(line)
                        i += 1

                # Save with "_offset" suffix
                base_name = os.path.basename(bdf_path)
                name_parts = os.path.splitext(base_name)
                output_name = name_parts[0] + "_offset" + name_parts[1]
                output_path = os.path.join(os.path.dirname(bdf_path), output_name)

                with open(output_path, 'w', encoding='latin-1') as f:
                    f.writelines(new_lines)

                self.log2(f"    Landing (ZOFFS): {landing_modified}, Bar (WA/WB): {bar_modified}")
                self.log2(f"    Saved: {output_name}")

                total_landing += landing_modified
                total_bar += bar_modified

            self.log2("\n" + "="*60)
            self.log2("OFFSET APPLICATION COMPLETED!")
            self.log2("="*60)
            self.log2(f"\nTotal across all files:")
            self.log2(f"  Landing elements (ZOFFS): {total_landing}")
            self.log2(f"  Bar elements (WA/WB): {total_bar}")
            self.log2(f"  Files processed: {len(self.run_bdfs)}")
            self.log2("  * INCLUDEs preserved (text-based modification)")
            self.log2("="*60)

            self.root.after(0, lambda: messagebox.showinfo("Success",
                f"Offsets applied to {len(self.run_bdfs)} BDF files!\n\n"
                f"Landing (ZOFFS): {total_landing}\nBar (WA/WB): {total_bar}\n\n"
                f"Files saved with '_offset' suffix\n"
                f"INCLUDEs preserved!"))

        except Exception as e:
            self.log2(f"\nERROR: {e}")
            import traceback
            self.log2(traceback.format_exc())
            self.root.after(0, lambda: messagebox.showerror("Error", str(e)))
        finally:
            self.root.after(0, lambda: [self.progress2.stop(),
                                       self.btn_apply_offset_tab2.config(state=tk.NORMAL)])


    def setup_tab3(self):
        """Tab 3: BDF Offset Tool"""
        main = ttk.Frame(self.tab3, padding="10")
        main.pack(fill=tk.BOTH, expand=True)

        ttk.Label(main, text="BDF Offset Tool v3.0", font=('Helvetica', 14, 'bold')).pack(pady=(0,10))

        # === INPUT BDF ===
        bf = ttk.LabelFrame(main, text="Input BDF File", padding="10")
        bf.pack(fill=tk.X, pady=5)
        ttk.Label(bf, text="BDF File:").grid(row=0, column=0, sticky=tk.W, padx=5)
        ttk.Entry(bf, textvariable=self.offset_input_bdf, width=60).grid(row=0, column=1, padx=5)
        ttk.Button(bf, text="Browse", command=self.browse_offset_bdf).grid(row=0, column=2, padx=5)

        # === ELEMENT IDS (EXCEL) ===
        ef = ttk.LabelFrame(main, text="Element IDs (Excel)", padding="10")
        ef.pack(fill=tk.X, pady=5)
        ttk.Label(ef, text="Excel File:").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)
        ttk.Entry(ef, textvariable=self.offset_element_excel, width=60).grid(row=0, column=1, padx=5, pady=5)
        ttk.Button(ef, text="Browse", command=self.browse_offset_excel).grid(row=0, column=2, padx=5, pady=5)

        ttk.Label(ef, text="Sheets: 'Landing_Offset' (Column A), 'Bar_Offset' (Column A)", 
                 font=('Helvetica', 9, 'italic')).grid(row=1, column=0, columnspan=3, pady=5)

        # === STEP 1: CALCULATE OFFSETS ===
        calc_frame = ttk.LabelFrame(main, text="Step 1: Calculate Offsets", padding="10")
        calc_frame.pack(fill=tk.X, pady=5)

        ttk.Label(calc_frame, text="Output CSV:").grid(row=0, column=0, sticky=tk.W, padx=5)
        ttk.Entry(calc_frame, textvariable=self.offset_csv_name, width=40).grid(row=0, column=1, sticky=tk.W, padx=5)

        self.btn_calc_offset = ttk.Button(calc_frame, text=">>> CALCULATE OFFSETS <<<", 
                                          command=self.start_calculate_offsets)
        self.btn_calc_offset.grid(row=1, column=0, columnspan=2, pady=10)

        # === STEP 2: APPLY OFFSETS ===
        apply_frame = ttk.LabelFrame(main, text="Step 2: Apply Offsets from CSV", padding="10")
        apply_frame.pack(fill=tk.X, pady=5)

        ttk.Label(apply_frame, text="Offset CSV:").grid(row=0, column=0, sticky=tk.W, padx=5)
        ttk.Entry(apply_frame, textvariable=self.offset_csv_path, width=60).grid(row=0, column=1, padx=5)
        ttk.Button(apply_frame, text="Browse", command=self.browse_offset_csv).grid(row=0, column=2, padx=5)

        ttk.Label(apply_frame, text="Output BDF:").grid(row=1, column=0, sticky=tk.W, padx=5, pady=5)
        ttk.Entry(apply_frame, textvariable=self.offset_output_name, width=40).grid(row=1, column=1, sticky=tk.W, padx=5, pady=5)

        self.btn_apply_offset = ttk.Button(apply_frame, text=">>> APPLY OFFSETS <<<", 
                                           command=self.start_apply_offsets)
        self.btn_apply_offset.grid(row=2, column=0, columnspan=2, pady=10)

        # === BUTTONS ===
        btnf = ttk.Frame(main)
        btnf.pack(fill=tk.X, pady=10)
        ttk.Button(btnf, text="Clear Log", command=self.clear_log3).pack(side=tk.LEFT, padx=5)

        self.progress3 = ttk.Progressbar(main, mode='indeterminate')
        self.progress3.pack(fill=tk.X, pady=5)

        # === LOG ===
        lf = ttk.LabelFrame(main, text="Log / Status", padding="10")
        lf.pack(fill=tk.BOTH, expand=True, pady=5)
        self.log_text3 = scrolledtext.ScrolledText(lf, height=12, wrap=tk.WORD)
        self.log_text3.pack(fill=tk.BOTH, expand=True)

        # Initial instructions
        self.log3("="*70)
        self.log3("BDF Offset Tool v3.0 - PyNastran Method")
        self.log3("="*70)
        self.log3("\nWorkflow:")
        self.log3("STEP 1: Calculate Offsets")
        self.log3("  - Select input BDF")
        self.log3("  - Select Excel with element IDs")
        self.log3("  - Click 'CALCULATE OFFSETS' -> Saves CSV")
        self.log3("")
        self.log3("STEP 2: Apply Offsets")
        self.log3("  - Select offset CSV from Step 1")
        self.log3("  - Click 'APPLY OFFSETS' -> Generates modified BDF")
        self.log3("")
        self.log3("Offset Logic:")
        self.log3("  Landing: zoffset = -thickness/2")
        self.log3("  Bar: wa=wb = offset_vector")
        self.log3("       magnitude = landing_t + bar_t/2")
        self.log3("="*70 + "\n")

    def browse_offset_bdf(self):
        f = filedialog.askopenfilename(
            title="Select BDF File",
            filetypes=[("BDF Files", "*.bdf *.dat *.nas"), ("All Files", "*.*")]
        )
        if f:
            self.offset_input_bdf.set(f)
            self.log3(f"Selected BDF: {os.path.basename(f)}")

    def browse_offset_excel(self):
        f = filedialog.askopenfilename(
            title="Select Excel File",
            filetypes=[("Excel Files", "*.xlsx *.xls"), ("All Files", "*.*")]
        )
        if f:
            self.offset_element_excel.set(f)
            self.log3(f"Selected Excel: {os.path.basename(f)}")

    def browse_offset_csv(self):
        f = filedialog.askopenfilename(
            title="Select Offset CSV File",
            filetypes=[("CSV Files", "*.csv"), ("All Files", "*.*")]
        )
        if f:
            self.offset_csv_path.set(f)
            self.log3(f"Selected CSV: {os.path.basename(f)}")

    def log3(self, msg):
        self.log_text3.insert(tk.END, msg + "\n")
        self.log_text3.see(tk.END)
        self.root.update_idletasks()

    def clear_log3(self):
        self.log_text3.delete(1.0, tk.END)

    def start_calculate_offsets(self):
        """Thread starter for offset calculation"""
        if not self.offset_input_bdf.get():
            messagebox.showerror("Error", "Please select BDF file")
            return
        if not self.offset_element_excel.get():
            messagebox.showerror("Error", "Please select Excel file with element IDs")
            return

        self.btn_calc_offset.config(state=tk.DISABLED)
        self.progress3.start()
        threading.Thread(target=self.calculate_offsets, daemon=True).start()

    def calculate_offsets(self):
        """Calculate offsets and save to CSV"""
        try:
            self.log3("\n" + "="*70)
            self.log3("STEP 1: CALCULATING OFFSETS")
            self.log3("="*70)

            # Read Excel for element IDs
            self.log3("\nReading element IDs from Excel...")
            xl = pd.ExcelFile(self.offset_element_excel.get())
            sheets = xl.sheet_names
            self.log3(f"Available sheets: {', '.join(sheets)}")

            landing_sheet = bar_sheet = None
            for s in sheets:
                s_lower = s.lower().replace('_', '').replace(' ', '')
                if 'landing' in s_lower and 'offset' in s_lower:
                    landing_sheet = s
                elif 'bar' in s_lower and 'offset' in s_lower:
                    bar_sheet = s

            landing_elem_ids = []
            bar_elem_ids = []

            if landing_sheet:
                self.log3(f"\nReading '{landing_sheet}'...")
                df = pd.read_excel(xl, sheet_name=landing_sheet)
                landing_elem_ids = df.iloc[:,0].dropna().astype(int).tolist()
                self.log3(f"  Found {len(landing_elem_ids)} landing element IDs")

            if bar_sheet:
                self.log3(f"\nReading '{bar_sheet}'...")
                df = pd.read_excel(xl, sheet_name=bar_sheet)
                bar_elem_ids = df.iloc[:,0].dropna().astype(int).tolist()
                self.log3(f"  Found {len(bar_elem_ids)} bar element IDs")

            # Read BDF with pyNastran
            self.log3("\n" + "="*70)
            self.log3("Reading BDF with pyNastran...")

            bdf = BDF(debug=False)
            bdf.read_bdf(self.offset_input_bdf.get(), validate=False, xref=True, 
                        read_includes=True, encoding='latin-1')

            self.log3(f"  Nodes: {len(bdf.nodes)}")
            self.log3(f"  Elements: {len(bdf.elements)}")
            self.log3(f"  Properties: {len(bdf.properties)}")

            # Calculate Landing offsets
            self.log3("\n" + "="*70)
            self.log3("Calculating Landing Offsets...")

            landing_results = []
            landing_thickness = {}
            landing_normals = {}

            for eid in landing_elem_ids:
                if eid in bdf.elements:
                    elem = bdf.elements[eid]
                    if hasattr(elem, 'pid') and elem.pid in bdf.properties:
                        prop = bdf.properties[elem.pid]

                        thickness = None
                        if hasattr(prop, 't'):  # PSHELL
                            thickness = prop.t
                        elif hasattr(prop, 'total_thickness'):  # PCOMP
                            thickness = prop.total_thickness()

                        if thickness:
                            zoffset = -thickness / 2.0
                            landing_thickness[eid] = thickness

                            landing_results.append({
                                'Element_ID': eid,
                                'Element_Type': elem.type,
                                'Property_ID': elem.pid,
                                'Property_Type': prop.type,
                                'Thickness': thickness,
                                'Zoffset': zoffset
                            })

                            # Calculate normal for bar calculations
                            if elem.type in ['CQUAD4', 'CTRIA3', 'CQUAD8', 'CTRIA6']:
                                node_ids = elem.node_ids[:4] if elem.type.startswith('CQUAD') else elem.node_ids[:3]
                                nodes = [bdf.nodes[nid] for nid in node_ids if nid in bdf.nodes]

                                if len(nodes) >= 3:
                                    p1 = np.array(nodes[0].get_position())
                                    p2 = np.array(nodes[1].get_position())
                                    p3 = np.array(nodes[2].get_position())

                                    v1 = p2 - p1
                                    v2 = p3 - p1
                                    normal = np.cross(v1, v2)
                                    normal_len = np.linalg.norm(normal)

                                    if normal_len > 1e-10:
                                        landing_normals[eid] = normal / normal_len

            self.log3(f"  Calculated offsets for {len(landing_results)} landing elements")

            # Build node -> shell mapping for bar calculations
            self.log3("\n" + "="*70)
            self.log3("Building node-to-shell mapping...")

            node_to_shells = {}
            for eid, elem in bdf.elements.items():
                if elem.type in ['CQUAD4', 'CTRIA3', 'CQUAD8', 'CTRIA6']:
                    for nid in elem.node_ids:
                        if nid not in node_to_shells:
                            node_to_shells[nid] = []
                        node_to_shells[nid].append(eid)

            self.log3(f"  Mapped {len(node_to_shells)} nodes to shell elements")

            # Calculate Bar offsets
            self.log3("\n" + "="*70)
            self.log3("Calculating Bar Offsets...")

            bar_results = []
            bar_thickness = {}
            bar_no_landing = 0

            for eid in bar_elem_ids:
                if eid in bdf.elements:
                    elem = bdf.elements[eid]
                    if elem.type == 'CBAR' and hasattr(elem, 'pid') and elem.pid in bdf.properties:
                        prop = bdf.properties[elem.pid]

                        thickness = None
                        if prop.type == 'PBARL':
                            if hasattr(prop, 'dim') and len(prop.dim) > 0:
                                thickness = prop.dim[0]
                        elif prop.type == 'PBAR':
                            if hasattr(prop, 'A') and prop.A > 0:
                                thickness = np.sqrt(prop.A)

                        if thickness:
                            bar_thickness[eid] = thickness

                            # Get bar nodes
                            bar_nodes = elem.node_ids[:2]

                            # Find connected landing elements
                            if bar_nodes[0] in node_to_shells and bar_nodes[1] in node_to_shells:
                                shells_n1 = set(node_to_shells[bar_nodes[0]])
                                shells_n2 = set(node_to_shells[bar_nodes[1]])
                                connected_shells = shells_n1.intersection(shells_n2)

                                # Find thickest landing
                                max_landing_thick = 0
                                landing_normal = None
                                connected_landing_id = None

                                for shell_eid in connected_shells:
                                    if shell_eid in landing_thickness:
                                        t = landing_thickness[shell_eid]
                                        if t > max_landing_thick:
                                            max_landing_thick = t
                                            connected_landing_id = shell_eid
                                            if shell_eid in landing_normals:
                                                landing_normal = landing_normals[shell_eid]

                                if landing_normal is not None and max_landing_thick > 0:
                                    # Calculate offset: landing_thickness + bar_thickness/2
                                    offset_magnitude = max_landing_thick + (thickness / 2.0)
                                    # Negative direction (opposite to landing normal)
                                    offset_vector = -landing_normal * offset_magnitude

                                    bar_results.append({
                                        'Element_ID': eid,
                                        'Element_Type': elem.type,
                                        'Property_ID': elem.pid,
                                        'Property_Type': prop.type,
                                        'Bar_Thickness': thickness,
                                        'Connected_Landing_ID': connected_landing_id,
                                        'Landing_Thickness': max_landing_thick,
                                        'Offset_Magnitude': offset_magnitude,
                                        'Offset_X': offset_vector[0],
                                        'Offset_Y': offset_vector[1],
                                        'Offset_Z': offset_vector[2]
                                    })
                                else:
                                    bar_no_landing += 1
                            else:
                                bar_no_landing += 1

            self.log3(f"  Calculated offsets for {len(bar_results)} bar elements")
            if bar_no_landing > 0:
                self.log3(f"  Skipped {bar_no_landing} bars (no landing connection)")

            # Save to CSV
            self.log3("\n" + "="*70)
            self.log3("Saving results to CSV...")

            output_dir = os.path.dirname(self.offset_input_bdf.get())
            csv_path = os.path.join(output_dir, self.offset_csv_name.get())

            with open(csv_path, 'w', newline='') as f:
                writer = csv.writer(f)

                # Write landing offsets
                writer.writerow(['LANDING OFFSETS'])
                writer.writerow(['Element_ID', 'Element_Type', 'Property_ID', 'Property_Type', 
                                'Thickness', 'Zoffset'])
                for row in landing_results:
                    writer.writerow([row['Element_ID'], row['Element_Type'], row['Property_ID'],
                                   row['Property_Type'], row['Thickness'], row['Zoffset']])

                writer.writerow([])  # Empty row

                # Write bar offsets
                writer.writerow(['BAR OFFSETS'])
                writer.writerow(['Element_ID', 'Element_Type', 'Property_ID', 'Property_Type',
                                'Bar_Thickness', 'Connected_Landing_ID', 'Landing_Thickness',
                                'Offset_Magnitude', 'Offset_X', 'Offset_Y', 'Offset_Z'])
                for row in bar_results:
                    writer.writerow([row['Element_ID'], row['Element_Type'], row['Property_ID'],
                                   row['Property_Type'], row['Bar_Thickness'], 
                                   row['Connected_Landing_ID'], row['Landing_Thickness'],
                                   row['Offset_Magnitude'], row['Offset_X'], 
                                   row['Offset_Y'], row['Offset_Z']])

            self.log3(f"\nSaved: {csv_path}")
            self.log3(f"File size: {os.path.getsize(csv_path) / 1024:.2f} KB")

            self.log3("\n" + "="*70)
            self.log3("CALCULATION COMPLETED!")
            self.log3("="*70)
            self.log3(f"\nSummary:")
            self.log3(f"  Landing elements: {len(landing_results)}")
            self.log3(f"  Bar elements: {len(bar_results)}")
            self.log3(f"  Output CSV: {os.path.basename(csv_path)}")
            self.log3("="*70)

            # Set CSV path for Step 2
            self.offset_csv_path.set(csv_path)

            self.root.after(0, lambda: messagebox.showinfo("Success",
                f"Offsets calculated!\n\nLanding: {len(landing_results)}\nBar: {len(bar_results)}\n\nCSV: {self.offset_csv_name.get()}"))

        except Exception as e:
            self.log3(f"\nERROR: {e}")
            import traceback
            self.log3(traceback.format_exc())
            self.root.after(0, lambda: messagebox.showerror("Error", str(e)))
        finally:
            self.root.after(0, lambda: [self.progress3.stop(), self.btn_calc_offset.config(state=tk.NORMAL)])

    def start_apply_offsets(self):
        """Thread starter for offset application"""
        if not self.offset_input_bdf.get():
            messagebox.showerror("Error", "Please select BDF file")
            return
        if not self.offset_csv_path.get():
            messagebox.showerror("Error", "Please select offset CSV file")
            return

        self.btn_apply_offset.config(state=tk.DISABLED)
        self.progress3.start()
        threading.Thread(target=self.apply_offsets, daemon=True).start()

    def apply_offsets(self):
        """Apply offsets from CSV to BDF - TEXT BASED (preserves INCLUDEs)"""
        try:
            self.log3("\n" + "="*70)
            self.log3("STEP 2: APPLYING OFFSETS FROM CSV (Text-based)")
            self.log3("="*70)

            # Read CSV
            self.log3("\nReading offset CSV...")

            landing_offsets = {}  # {eid: zoffset}
            bar_offsets = {}      # {eid: (offset_x, offset_y, offset_z)}

            with open(self.offset_csv_path.get(), 'r') as f:
                reader = csv.reader(f)
                section = None

                for row in reader:
                    if not row or not row[0]:
                        continue

                    if 'LANDING OFFSETS' in row[0]:
                        section = 'landing'
                        next(reader)  # Skip header
                        continue
                    elif 'BAR OFFSETS' in row[0]:
                        section = 'bar'
                        next(reader)  # Skip header
                        continue

                    if section == 'landing':
                        try:
                            eid = int(row[0])
                            zoffset = float(row[5])
                            landing_offsets[eid] = zoffset
                        except:
                            pass
                    elif section == 'bar':
                        try:
                            eid = int(row[0])
                            offset_x = float(row[8])
                            offset_y = float(row[9])
                            offset_z = float(row[10])
                            bar_offsets[eid] = (offset_x, offset_y, offset_z)
                        except:
                            pass

            self.log3(f"  Loaded {len(landing_offsets)} landing offsets")
            self.log3(f"  Loaded {len(bar_offsets)} bar offsets")

            # Helper function to format field (8-char fixed width)
            def fmt_field(value, width=8):
                if isinstance(value, float):
                    s = f"{value:.4f}"
                    if len(s) > width:
                        s = f"{value:.2E}"
                    return s[:width].ljust(width)
                return str(value)[:width].ljust(width)

            # Read BDF as text
            self.log3("\n" + "="*70)
            self.log3("Reading BDF as text (preserves INCLUDEs)...")

            bdf_path = self.offset_input_bdf.get()
            with open(bdf_path, 'r', encoding='latin-1') as f:
                lines = f.readlines()

            self.log3(f"  Total lines: {len(lines)}")

            # Apply offsets using text replacement
            new_lines = []
            i = 0
            landing_modified = 0
            bar_modified = 0

            while i < len(lines):
                line = lines[i]

                # Check for CQUAD4 (landing/skin elements with zoffset)
                if line.startswith('CQUAD4'):
                    try:
                        eid = int(line[8:16].strip())
                        if eid in landing_offsets:
                            zoff = landing_offsets[eid]
                            if len(line) >= 64:
                                new_line = line[:64] + fmt_field(zoff) + line[72:] if len(line) > 72 else line[:64] + fmt_field(zoff) + '\n'
                                new_lines.append(new_line)
                                landing_modified += 1
                                i += 1
                                continue
                    except:
                        pass
                    new_lines.append(line)
                    i += 1
                    continue

                # Check for CBAR elements
                elif line.startswith('CBAR'):
                    try:
                        eid = int(line[8:16].strip())
                        if eid in bar_offsets:
                            offset_vec = bar_offsets[eid]

                            # Check if next line is continuation
                            if i + 1 < len(lines) and (lines[i+1].startswith('+') or lines[i+1].startswith('*') or lines[i+1].startswith(' ')):
                                cont_line = lines[i+1]
                                new_cont = cont_line[:24]
                                new_cont += fmt_field(offset_vec[0])
                                new_cont += fmt_field(offset_vec[1])
                                new_cont += fmt_field(offset_vec[2])
                                new_cont += fmt_field(offset_vec[0])
                                new_cont += fmt_field(offset_vec[1])
                                new_cont += fmt_field(offset_vec[2])
                                new_cont += '\n'

                                new_lines.append(line)
                                new_lines.append(new_cont)
                                bar_modified += 1
                                i += 2
                                continue
                            else:
                                new_lines.append(line.rstrip() + '+CB' + str(eid)[-4:] + '\n')
                                cont_name = '+CB' + str(eid)[-4:]
                                new_cont = cont_name.ljust(8)
                                new_cont += '        '
                                new_cont += '        '
                                new_cont += fmt_field(offset_vec[0])
                                new_cont += fmt_field(offset_vec[1])
                                new_cont += fmt_field(offset_vec[2])
                                new_cont += fmt_field(offset_vec[0])
                                new_cont += fmt_field(offset_vec[1])
                                new_cont += fmt_field(offset_vec[2])
                                new_cont += '\n'
                                new_lines.append(new_cont)
                                bar_modified += 1
                                i += 1
                                continue
                    except:
                        pass
                    new_lines.append(line)
                    i += 1
                    continue

                else:
                    new_lines.append(line)
                    i += 1

            self.log3(f"\n[1] Landing Offsets (ZOFFS): {landing_modified}")
            self.log3(f"[2] Bar Offsets (WA/WB): {bar_modified}")

            # Write output BDF
            self.log3("\n[3] Writing output BDF...")
            output_dir = os.path.dirname(bdf_path)
            output_path = os.path.join(output_dir, self.offset_output_name.get())

            with open(output_path, 'w', encoding='latin-1') as f:
                f.writelines(new_lines)

            self.log3(f"\nOutput: {output_path}")
            self.log3(f"File size: {os.path.getsize(output_path) / (1024*1024):.2f} MB")

            self.log3("\n" + "="*70)
            self.log3("APPLICATION COMPLETED!")
            self.log3("="*70)
            self.log3(f"\nSummary:")
            self.log3(f"  Landing elements (ZOFFS): {landing_modified}")
            self.log3(f"  Bar elements (WA/WB): {bar_modified}")
            self.log3(f"  Output: {os.path.basename(output_path)}")
            self.log3("  * INCLUDEs preserved (text-based modification)")
            self.log3("="*70)

            self.root.after(0, lambda: messagebox.showinfo("Success",
                f"Offsets applied!\n\nLanding (ZOFFS): {landing_modified}\nBar (WA/WB): {bar_modified}\n\nOutput: {self.offset_output_name.get()}\n\nINCLUDEs preserved!"))

        except Exception as e:
            self.log3(f"\nERROR: {e}")
            import traceback
            self.log3(traceback.format_exc())
            self.root.after(0, lambda: messagebox.showerror("Error", str(e)))
        finally:
            self.root.after(0, lambda: [self.progress3.stop(), self.btn_apply_offset.config(state=tk.NORMAL)])


    def start_full_run(self):
        if not self.run_bdfs:
            messagebox.showerror("Error","Add BDF files"); return
        self.btn_full.config(state=tk.DISABLED)
        self.progress2.start()
        threading.Thread(target=self.do_full_run, daemon=True).start()
    
    def do_full_run(self):
        try:
            self.log2("="*60)
            self.log2("FULL RUN (All 4 Steps)")
            self.log2("="*60)
            out_folder = self.run_output_folder.get()
            os.makedirs(out_folder, exist_ok=True)
            
            # === STEP 1: Update Properties ===
            self.log2("\n>>> STEP 1: Update Properties")
            for bdf_path in self.run_bdfs:
                self.log2(f"  {os.path.basename(bdf_path)}")
                out_bdf = self.copy_bdf_to_output(bdf_path, out_folder)
                stats, _ = self.update_properties_in_file(out_bdf)
                self.log2(f"    PBARL={stats['pbarl']} PBAR={stats['pbar']} PSHELL={stats['pshell']} PCOMP={stats['pcomp']}")
            
            # === STEP 2: Run Nastran ===
            if self.nastran_path.get():
                self.log2("\n>>> STEP 2: Run Nastran")
                nastran = self.nastran_path.get()
                import subprocess
                import time
                
                bdf_files_in_output = [f for f in os.listdir(out_folder) if f.lower().endswith(('.bdf','.dat','.nas'))]
                
                for f in bdf_files_in_output:
                    bdf_full_path = os.path.join(out_folder, f)
                    self.log2(f"  Running: {f}")
                    
                    # Nastran'ı çalıştır ve bitene kadar bekle
                    try:
                        cmd = f'"{nastran}" "{bdf_full_path}" out="{out_folder}" scratch=yes batch=no'
                        process = subprocess.Popen(cmd, shell=True)
                        process.wait()  # Bitene kadar bekle
                        self.log2(f"    Completed: {f}")
                    except Exception as e:
                        self.log2(f"    Error running {f}: {e}")
                
                # OP2 dosyalarının oluşmasını bekle
                self.log2("  Waiting for OP2 files...")
                time.sleep(2)
            else:
                self.log2("\n>>> STEP 2: SKIPPED (No Nastran path)")
            
            # === STEP 3: Post-Process OP2 ===
            self.log2("\n>>> STEP 3: Post-Process OP2")
            op2_files = [os.path.join(out_folder, f) for f in os.listdir(out_folder) if f.lower().endswith('.op2')]
            
            if op2_files:
                # Element -> Property mapping ve PBARL dimensions
                elem_prop = {}
                pbarl_dims = {}
                
                # BDF'lerden oku
                bdf_files_to_read = list(self.run_bdfs)
                for f in os.listdir(out_folder):
                    if f.lower().endswith(('.bdf', '.dat', '.nas')):
                        bdf_files_to_read.append(os.path.join(out_folder, f))
                
                for bdf_path in bdf_files_to_read:
                    try:
                        bdf = BDF(debug=False)
                        bdf.read_bdf(bdf_path, validate=False, xref=False, read_includes=True)
                        
                        for eid, el in bdf.elements.items():
                            if hasattr(el, 'pid'):
                                elem_prop[eid] = el.pid
                        
                        for pid, prop in bdf.properties.items():
                            prop_type = prop.type
                            if prop_type == 'PBARL':
                                dims = prop.dim if hasattr(prop, 'dim') else []
                                bar_type = prop.bar_type if hasattr(prop, 'bar_type') else 'UNKNOWN'
                                if len(dims) >= 2:
                                    pbarl_dims[pid] = {'dim1': dims[0], 'dim2': dims[1], 'type': bar_type}
                                elif len(dims) == 1:
                                    pbarl_dims[pid] = {'dim1': dims[0], 'dim2': dims[0], 'type': bar_type}
                            elif prop_type == 'PBAR':
                                area = prop.A if hasattr(prop, 'A') else None
                                if area:
                                    import math
                                    side = math.sqrt(area) if area > 0 else 0
                                    pbarl_dims[pid] = {'dim1': side, 'dim2': side, 'type': 'PBAR', 'area': area}
                    except:
                        pass
                
                self.log2(f"  Elements: {len(elem_prop)}, Bar properties: {len(pbarl_dims)}")
                
                results = []
                for op2_path in op2_files:
                    self.log2(f"  Processing: {os.path.basename(op2_path)}")
                    try:
                        op2 = OP2(debug=False)
                        op2.read_op2(op2_path)
                        if hasattr(op2, 'cbar_force') and op2.cbar_force:
                            for sc_id, force in op2.cbar_force.items():
                                for i, eid in enumerate(force.element):
                                    axial = force.data[0,i,6] if len(force.data.shape)==3 else force.data[i,6]
                                    pid = elem_prop.get(eid)
                                    d1 = d2 = area = stress = None
                                    
                                    if pid and pid in self.bar_properties:
                                        d1 = self.bar_properties[pid]['dim1']
                                        d2 = self.bar_properties[pid]['dim2']
                                        area = d1 * d2
                                        if area > 0: stress = axial / area
                                    elif pid and pid in pbarl_dims:
                                        prop_info = pbarl_dims[pid]
                                        d1 = prop_info['dim1']
                                        d2 = prop_info['dim2']
                                        area = prop_info.get('area', d1 * d2)
                                        if area > 0: stress = axial / area
                                    
                                    results.append({'OP2': os.path.basename(op2_path), 'Subcase': sc_id, 'Element': eid,
                                        'Property': pid, 'Axial': axial, 'Dim1': d1, 'Dim2': d2, 'Area': area, 'Stress': stress})
                    except Exception as e:
                        self.log2(f"    Error: {e}")
                
                csv_path = os.path.join(out_folder, self.csv_output_name.get())
                with open(csv_path, 'w', newline='') as f:
                    w = csv.DictWriter(f, fieldnames=['OP2','Subcase','Element','Property','Axial','Dim1','Dim2','Area','Stress'])
                    w.writeheader()
                    w.writerows(results)
                self.log2(f"  Saved: {self.csv_output_name.get()} ({len(results)} rows)")
            else:
                self.log2("  No OP2 files found!")
            
            # === STEP 4: Combine Stress ===
            self.log2("\n>>> STEP 4: Combine Stress")
            if self.residual_strength_df is not None:
                stress_csv = os.path.join(out_folder, self.csv_output_name.get())
                if os.path.exists(stress_csv):
                    stress_df = pd.read_csv(stress_csv)
                    lookup = {}
                    for _, row in stress_df.iterrows():
                        key = (int(row['Subcase']), int(row['Element']))
                        lookup[key] = row['Stress'] if pd.notna(row['Stress']) else 0
                    elements = stress_df['Element'].unique()
                    
                    rs_df = self.residual_strength_df
                    cols = rs_df.columns.tolist()
                    comb_col = cols[0]
                    
                    comp_cols = []
                    idx = 1
                    while idx < len(cols) - 1:
                        col_name = str(cols[idx]).upper()
                        next_col_name = str(cols[idx+1]).upper()
                        if ('CASE' in col_name or 'ID' in col_name) and 'MULT' in next_col_name:
                            comp_cols.append((cols[idx], cols[idx+1]))
                            idx += 2
                        else:
                            idx += 1
                    
                    comb_results = []
                    for _, rs_row in rs_df.iterrows():
                        comb_lc = rs_row[comb_col]
                        if pd.isna(comb_lc): continue
                        comb_lc = int(comb_lc)
                        
                        for eid in elements:
                            total_stress = 0.0
                            components = []
                            for case_col, mult_col in comp_cols:
                                case_id = rs_row[case_col]
                                multiplier = rs_row[mult_col]
                                if pd.isna(case_id) or pd.isna(multiplier): continue
                                case_id = int(case_id)
                                multiplier = float(multiplier)
                                key = (case_id, int(eid))
                                if key in lookup:
                                    stress_val = lookup[key]
                                    if stress_val is not None:
                                        total_stress += stress_val * multiplier
                                        components.append(f"{case_id}*{multiplier}")
                            if components:
                                comb_results.append({'Combined_LC': comb_lc, 'Element': eid, 
                                    'Combined_Stress': total_stress, 'Components': ' + '.join(components)})
                    
                    comb_csv = os.path.join(out_folder, self.combined_csv_name.get())
                    with open(comb_csv, 'w', newline='') as f:
                        w = csv.DictWriter(f, fieldnames=['Combined_LC','Element','Combined_Stress','Components'])
                        w.writeheader()
                        w.writerows(comb_results)
                    self.log2(f"  Saved: {self.combined_csv_name.get()} ({len(comb_results)} rows)")
                else:
                    self.log2("  Stress CSV not found!")
            else:
                self.log2("  SKIPPED (No Residual Strength data loaded)")
            
            self.log2("\n" + "="*60)
            self.log2("FULL RUN COMPLETED!")
            self.log2("="*60)
            self.root.after(0, lambda: messagebox.showinfo("Done","Full run completed!"))
        except Exception as e:
            self.log2(f"ERROR: {e}")
            import traceback
            self.log2(traceback.format_exc())
        finally:
            self.root.after(0, lambda: [self.progress2.stop(), self.btn_full.config(state=tk.NORMAL)])



    # ========================================================================
    # TAB 4: RF CHECK TOOL - Complete Integration
    # All methods from rf_check_tool_v2_1.txt
    # ========================================================================
    def setup_tab4(self):
        canvas = tk.Canvas(self.tab4)
        scrollbar = ttk.Scrollbar(self.tab4, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind("<Configure>", lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        main = scrollable_frame
        
        ttk.Label(main, text="RF Check Tool v2.1", font=('Helvetica', 16, 'bold')).pack(pady=(10, 5))
        ttk.Label(main, text="Process BDF Tool v10 outputs and calculate Reserve Factors + Required Thickness", 
                  font=('Helvetica', 10), foreground='gray').pack(pady=(0, 10))
        
        # Section 1: Input Files
        input_frame = ttk.LabelFrame(main, text="1. Input Files (from BDF Tool v10 Tab 2)", padding="10")
        input_frame.pack(fill=tk.X, pady=5, padx=10)
        
        row1 = ttk.Frame(input_frame)
        row1.pack(fill=tk.X, pady=2)
        ttk.Label(row1, text="bar_stress_results.csv:", width=25).pack(side=tk.LEFT)
        ttk.Entry(row1, textvariable=self.bar_stress_path, width=55).pack(side=tk.LEFT, padx=5)
        ttk.Button(row1, text="Browse", command=self.rf_browse_bar_stress).pack(side=tk.LEFT, padx=2)
        ttk.Button(row1, text="Load", command=self.rf_load_bar_stress).pack(side=tk.LEFT, padx=2)
        
        self.bar_stress_status = ttk.Label(input_frame, text="Not loaded", foreground="gray")
        self.bar_stress_status.pack(anchor=tk.W, pady=2)
        
        row2 = ttk.Frame(input_frame)
        row2.pack(fill=tk.X, pady=2)
        ttk.Label(row2, text="combined_stress_results.csv:", width=25).pack(side=tk.LEFT)
        ttk.Entry(row2, textvariable=self.combined_stress_path, width=55).pack(side=tk.LEFT, padx=5)
        ttk.Button(row2, text="Browse", command=self.rf_browse_combined_stress).pack(side=tk.LEFT, padx=2)
        ttk.Button(row2, text="Load", command=self.rf_load_combined_stress).pack(side=tk.LEFT, padx=2)
        
        self.combined_stress_status = ttk.Label(input_frame, text="Not loaded", foreground="gray")
        self.combined_stress_status.pack(anchor=tk.W, pady=2)
        
        # Section 2: Merge
        merge_frame = ttk.LabelFrame(main, text="2. Merge Data & Create Max Stress Summary", padding="10")
        merge_frame.pack(fill=tk.X, pady=5, padx=10)
        
        ttk.Label(merge_frame, text="Merges Property (via Element), then Dim1/Dim2 (via Property),\n"
                  "then creates max_stress_summary grouped by Property.", 
                  font=('Helvetica', 9), foreground='gray').pack(anchor=tk.W, pady=5)
        
        ttk.Button(merge_frame, text="Merge & Create Summary", command=self.rf_merge_and_create_summary).pack(anchor=tk.W, pady=5)
        
        self.merge_status = ttk.Label(merge_frame, text="Not processed", foreground="gray")
        self.merge_status.pack(anchor=tk.W, pady=2)
        
        # Section 3: Allowable
        allow_frame = ttk.LabelFrame(main, text="3. Allowable Stress Data", padding="10")
        allow_frame.pack(fill=tk.X, pady=5, padx=10)
        
        row3 = ttk.Frame(allow_frame)
        row3.pack(fill=tk.X, pady=2)
        ttk.Label(row3, text="Allowable CSV/Excel:", width=25).pack(side=tk.LEFT)
        ttk.Entry(row3, textvariable=self.allowable_path, width=55).pack(side=tk.LEFT, padx=5)
        ttk.Button(row3, text="Browse", command=self.rf_browse_allowable).pack(side=tk.LEFT, padx=2)
        ttk.Button(row3, text="Load & Fit", command=self.rf_load_allowable).pack(side=tk.LEFT, padx=2)
        
        settings_row = ttk.Frame(allow_frame)
        settings_row.pack(fill=tk.X, pady=5)
        ttk.Label(settings_row, text="R² Threshold:").pack(side=tk.LEFT)
        ttk.Entry(settings_row, textvariable=self.r2_threshold_var, width=8).pack(side=tk.LEFT, padx=5)
        ttk.Label(settings_row, text="Min Data Points:").pack(side=tk.LEFT, padx=(20, 0))
        ttk.Entry(settings_row, textvariable=self.min_data_points_var, width=8).pack(side=tk.LEFT, padx=5)
        
        self.allowable_status = ttk.Label(allow_frame, text="Not loaded", foreground="gray")
        self.allowable_status.pack(anchor=tk.W, pady=2)
        
        # Section 4: RF Settings
        rf_frame = ttk.LabelFrame(main, text="4. RF Calculation Settings", padding="10")
        rf_frame.pack(fill=tk.X, pady=5, padx=10)
        
        rf_row = ttk.Frame(rf_frame)
        rf_row.pack(fill=tk.X, pady=5)
        ttk.Label(rf_row, text="Minimum RF Target:").pack(side=tk.LEFT)
        ttk.Entry(rf_row, textvariable=self.min_rf_var, width=10).pack(side=tk.LEFT, padx=5)
        ttk.Label(rf_row, text="(RF = Allowable / |Stress|, must be >= Min RF to PASS)", 
                  foreground='gray').pack(side=tk.LEFT, padx=10)
        
        # Section 5: Output
        output_frame = ttk.LabelFrame(main, text="5. Output", padding="10")
        output_frame.pack(fill=tk.X, pady=5, padx=10)
        
        out_row = ttk.Frame(output_frame)
        out_row.pack(fill=tk.X, pady=2)
        ttk.Label(out_row, text="Output Folder:", width=25).pack(side=tk.LEFT)
        ttk.Entry(out_row, textvariable=self.output_folder, width=55).pack(side=tk.LEFT, padx=5)
        ttk.Button(out_row, text="Browse", command=self.rf_browse_output).pack(side=tk.LEFT, padx=2)
        
        # Section 6: Actions
        action_frame = ttk.LabelFrame(main, text="6. Actions", padding="10")
        action_frame.pack(fill=tk.X, pady=5, padx=10)
        
        btn_row = ttk.Frame(action_frame)
        btn_row.pack(fill=tk.X, pady=5)
        
        ttk.Button(btn_row, text=">>> Calculate RF <<<", command=self.rf_calculate_rf, width=20).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_row, text="Export Results", command=self.rf_export_results, width=15).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_row, text="Clear Log", command=self.rf_clear_log).pack(side=tk.LEFT, padx=5)
        
        self.progress = ttk.Progressbar(action_frame, mode='determinate')
        self.progress.pack(fill=tk.X, pady=5)
        
        self.progress_label = ttk.Label(action_frame, text="Ready")
        self.progress_label.pack(anchor=tk.W)
        
        # Section 7: Results
        result_frame = ttk.LabelFrame(main, text="7. Results Summary", padding="10")
        result_frame.pack(fill=tk.X, pady=5, padx=10)
        
        self.result_summary = ttk.Label(result_frame, text="Run calculation to see results", 
                                         font=('Helvetica', 11, 'bold'), foreground='blue')
        self.result_summary.pack(anchor=tk.W, pady=5)
        
        # Section 8: Log
        log_frame = ttk.LabelFrame(main, text="8. Log", padding="10")
        log_frame.pack(fill=tk.BOTH, expand=True, pady=5, padx=10)
        
        self.rf_log_text = scrolledtext.ScrolledText(log_frame, height=18, font=('Courier', 9))
        self.rf_log_text.pack(fill=tk.BOTH, expand=True)
    
    def rf_log(self, msg):
        self.rf_log_text.insert(tk.END, msg + "\n")
        self.rf_log_text.see(tk.END)
        self.root.update_idletasks()
    
    def rf_clear_log(self):
        self.rf_log_text.delete(1.0, tk.END)
    
    def rf_update_progress(self, value, text=""):
        self.progress['value'] = value
        self.progress_label.config(text=text)
        self.root.update_idletasks()
    
    def rf_browse_bar_stress(self):
        f = filedialog.askopenfilename(filetypes=[("CSV", "*.csv"), ("All", "*.*")])
        if f:
            self.bar_stress_path.set(f)
    
    def rf_browse_combined_stress(self):
        f = filedialog.askopenfilename(filetypes=[("CSV", "*.csv"), ("All", "*.*")])
        if f:
            self.combined_stress_path.set(f)
    
    def rf_browse_allowable(self):
        f = filedialog.askopenfilename(filetypes=[("CSV/Excel", "*.csv *.xlsx *.xls"), ("All", "*.*")])
        if f:
            self.allowable_path.set(f)
    
    def rf_browse_output(self):
        f = filedialog.askdirectory()
        if f:
            self.output_folder.set(f)
    
    def rf_load_bar_stress(self):
        path = self.bar_stress_path.get()
        if not path or not os.path.exists(path):
            messagebox.showerror("Error", "Select a valid bar_stress_results.csv file")
            return
        
        try:
            self.bar_stress_df = pd.read_csv(path)
            
            n_rows = len(self.bar_stress_df)
            n_props = self.bar_stress_df['Property'].nunique() if 'Property' in self.bar_stress_df.columns else 0
            n_elems = self.bar_stress_df['Element'].nunique() if 'Element' in self.bar_stress_df.columns else 0
            
            self.bar_stress_status.config(
                text=f"✓ Loaded: {n_rows} rows, {n_props} properties, {n_elems} elements",
                foreground="green"
            )
            
            self.rf_log(f"Loaded bar_stress_results.csv:")
            self.rf_log(f"  Rows: {n_rows}, Properties: {n_props}, Elements: {n_elems}")
            self.rf_log(f"  Columns: {list(self.bar_stress_df.columns)}")
            
            if not self.output_folder.get():
                self.output_folder.set(os.path.dirname(path))
            
        except Exception as e:
            self.bar_stress_status.config(text=f"Error: {e}", foreground="red")
            messagebox.showerror("Error", str(e))
    
    def rf_load_combined_stress(self):
        path = self.combined_stress_path.get()
        if not path or not os.path.exists(path):
            messagebox.showerror("Error", "Select a valid combined_stress_results.csv file")
            return
        
        try:
            self.combined_stress_df = pd.read_csv(path)
            
            n_rows = len(self.combined_stress_df)
            n_lc = self.combined_stress_df['Combined_LC'].nunique() if 'Combined_LC' in self.combined_stress_df.columns else 0
            n_elems = self.combined_stress_df['Element'].nunique() if 'Element' in self.combined_stress_df.columns else 0
            
            self.combined_stress_status.config(
                text=f"✓ Loaded: {n_rows} rows, {n_lc} load cases, {n_elems} elements",
                foreground="green"
            )
            
            self.rf_log(f"\nLoaded combined_stress_results.csv:")
            self.rf_log(f"  Rows: {n_rows}, Load Cases: {n_lc}, Elements: {n_elems}")
            self.rf_log(f"  Columns: {list(self.combined_stress_df.columns)}")
            
        except Exception as e:
            self.combined_stress_status.config(text=f"Error: {e}", foreground="red")
            messagebox.showerror("Error", str(e))
    
    def rf_merge_and_create_summary(self):
        """
        Merge Property (via Element), then Dim1/Dim2 (via Property).
        Then create max_stress_summary grouped by Property AND Element.
        """
        self.rf_log("\n" + "="*70)
        self.rf_log("MERGE DATA & CREATE MAX STRESS SUMMARY")
        self.rf_log("="*70)

        if self.combined_stress_df is None:
            messagebox.showerror("Error", "Load combined_stress file first")
            return

        if self.bar_stress_df is None:
            messagebox.showerror("Error", "Load bar_stress file first")
            return

        # Determine output folder
        out_dir = self.output_folder.get()
        if not out_dir:
            # Use bar_stress file directory as default
            bar_path = self.bar_stress_path.get()
            if bar_path:
                out_dir = os.path.dirname(bar_path)
                self.output_folder.set(out_dir)
            else:
                out_dir = filedialog.askdirectory(title="Select Output Folder")
                if not out_dir:
                    messagebox.showerror("Error", "Please select an output folder")
                    return
                self.output_folder.set(out_dir)

        os.makedirs(out_dir, exist_ok=True)
        self.rf_log(f"Output folder: {out_dir}")

        self.rf_update_progress(10, "Merging data...")

        try:
            combined = self.combined_stress_df.copy()
            bar = self.bar_stress_df.copy()

            self.rf_log(f"\nCombined stress: {len(combined)} rows, columns: {list(combined.columns)}")
            self.rf_log(f"Bar stress: {len(bar)} rows, columns: {list(bar.columns)}")

            # =========================================================
            # STEP 1: Create Element -> Property lookup
            # =========================================================
            self.rf_log("\n--- STEP 1: Element -> Property lookup ---")

            elem_to_prop = bar.groupby('Element')['Property'].first().reset_index()
            self.rf_log(f"  Unique Element->Property: {len(elem_to_prop)}")

            # =========================================================
            # STEP 2: Create Property -> Dim1, Dim2, Area lookup
            # =========================================================
            self.rf_log("\n--- STEP 2: Property -> Dim1/Dim2 lookup ---")

            prop_cols = ['Property']
            for col in ['Dim1', 'Dim2', 'Area']:
                if col in bar.columns:
                    prop_cols.append(col)

            prop_lookup = bar.groupby('Property').first().reset_index()[prop_cols]
            self.rf_log(f"  Unique Property->Dim: {len(prop_lookup)}")

            # Sample
            self.rf_log(f"\n  Sample Property lookup:")
            for _, row in prop_lookup.head(3).iterrows():
                self.rf_log(f"    Property {int(row['Property'])}: Dim1={row.get('Dim1', 'N/A')}, Dim2={row.get('Dim2', 'N/A')}")

            # =========================================================
            # STEP 3: Merge Property into combined using Element
            # =========================================================
            self.rf_log("\n--- STEP 3: Merge Property (via Element) ---")

            if 'Property' not in combined.columns:
                merged = combined.merge(elem_to_prop, on='Element', how='left')
                n_prop = merged['Property'].notna().sum()
                self.rf_log(f"  Merged Property: {n_prop}/{len(merged)} rows")
            else:
                merged = combined.copy()
                self.rf_log(f"  Property already exists")

            # =========================================================
            # STEP 4: Merge Dim1, Dim2 using Property
            # =========================================================
            self.rf_log("\n--- STEP 4: Merge Dim1/Dim2 (via Property) ---")

            dim_cols = [c for c in prop_cols if c != 'Property' and c not in merged.columns]

            if dim_cols:
                self.rf_log(f"  Adding columns: {dim_cols}")
                merge_cols = ['Property'] + dim_cols
                merged = merged.merge(prop_lookup[merge_cols], on='Property', how='left')

                for col in dim_cols:
                    if col in merged.columns:
                        n_filled = merged[col].notna().sum()
                        self.rf_log(f"    {col}: {n_filled}/{len(merged)} filled")
            else:
                self.rf_log(f"  All Dim columns already exist")

            # Verify merge
            self.rf_log(f"\n  Verification (first 3 rows):")
            for _, row in merged.head(3).iterrows():
                elem = int(row['Element']) if pd.notna(row.get('Element')) else 'N/A'
                prop = int(row['Property']) if pd.notna(row.get('Property')) else 'N/A'
                dim1 = row.get('Dim1', 'N/A')
                dim2 = row.get('Dim2', 'N/A')
                self.rf_log(f"    Element {elem}: Property={prop}, Dim1={dim1}, Dim2={dim2}")

            self.merged_combined_df = merged.copy()

            # =========================================================
            # STEP 5A: Create Max Stress Summary by Property
            # =========================================================
            self.rf_update_progress(50, "Creating max stress summary (Property)...")
            self.rf_log("\n--- STEP 5A: Max Stress Summary (Property-based) ---")

            stress_col = 'Combined_Stress' if 'Combined_Stress' in merged.columns else 'Stress'
            self.rf_log(f"  Stress column: {stress_col}")

            # Filter valid Property
            valid_mask = merged['Property'].notna()
            merged_valid = merged[valid_mask].copy()
            self.rf_log(f"  Valid rows: {len(merged_valid)}")

            # Absolute stress
            merged_valid['Abs_Stress'] = merged_valid[stress_col].abs()

            # Max stress per property
            idx_max = merged_valid.groupby('Property')['Abs_Stress'].idxmax()
            max_rows = merged_valid.loc[idx_max].copy()
            self.rf_log(f"  Unique properties: {len(max_rows)}")

            # Min stress per property
            min_stress = merged_valid.groupby('Property')[stress_col].min().reset_index()
            min_stress.columns = ['Property', 'Min_Stress']

            max_stress_summary = max_rows.merge(min_stress, on='Property', how='left')

            # Rename
            if stress_col in max_stress_summary.columns:
                max_stress_summary = max_stress_summary.rename(columns={stress_col: 'Max_Stress'})

            # Remove temp
            if 'Abs_Stress' in max_stress_summary.columns:
                max_stress_summary = max_stress_summary.drop(columns=['Abs_Stress'])

            # Reorder columns
            priority = ['Property', 'Dim1', 'Dim2', 'Area', 'Max_Stress', 'Min_Stress', 'Element', 'Combined_LC']
            final_cols = [c for c in priority if c in max_stress_summary.columns]
            other_cols = [c for c in max_stress_summary.columns if c not in final_cols]
            max_stress_summary = max_stress_summary[final_cols + other_cols]

            self.max_stress_df = max_stress_summary

            # =========================================================
            # STEP 5B: Create Max Stress Summary by Element
            # =========================================================
            self.rf_update_progress(60, "Creating max stress summary (Element)...")
            self.rf_log("\n--- STEP 5B: Max Stress Summary (Element-based) ---")

            # Max stress per element
            idx_max_elem = merged_valid.groupby('Element')['Abs_Stress'].idxmax()
            max_rows_elem = merged_valid.loc[idx_max_elem].copy()
            self.rf_log(f"  Unique elements: {len(max_rows_elem)}")

            # Min stress per element
            min_stress_elem = merged_valid.groupby('Element')[stress_col].min().reset_index()
            min_stress_elem.columns = ['Element', 'Min_Stress']

            max_stress_elem_summary = max_rows_elem.merge(min_stress_elem, on='Element', how='left')

            # Rename
            if stress_col in max_stress_elem_summary.columns:
                max_stress_elem_summary = max_stress_elem_summary.rename(columns={stress_col: 'Max_Stress'})

            # Remove temp
            if 'Abs_Stress' in max_stress_elem_summary.columns:
                max_stress_elem_summary = max_stress_elem_summary.drop(columns=['Abs_Stress'])

            # Reorder columns
            priority_elem = ['Element', 'Property', 'Dim1', 'Dim2', 'Area', 'Max_Stress', 'Min_Stress', 'Combined_LC']
            final_cols_elem = [c for c in priority_elem if c in max_stress_elem_summary.columns]
            other_cols_elem = [c for c in max_stress_elem_summary.columns if c not in final_cols_elem]
            max_stress_elem_summary = max_stress_elem_summary[final_cols_elem + other_cols_elem]

            self.max_stress_elem_df = max_stress_elem_summary

            # =========================================================
            # STEP 6: Save and Report
            # =========================================================
            self.rf_update_progress(80, "Saving files...")
            self.rf_log("\n--- STEP 6: Results ---")

            n_props = len(max_stress_summary)
            n_elems = len(max_stress_elem_summary)
            self.rf_log(f"  Max Stress Summary (Property): {n_props} properties")
            self.rf_log(f"  Max Stress Summary (Element): {n_elems} elements")

            self.rf_log(f"\n  Sample Property results:")
            self.rf_log(f"  {'Property':<12} {'Dim1':>8} {'Dim2':>8} {'Max_Stress':>12}")
            self.rf_log("  " + "-"*50)
            for _, row in max_stress_summary.head(5).iterrows():
                prop = int(row['Property']) if pd.notna(row.get('Property')) else 'N/A'
                dim1 = row.get('Dim1', 0)
                dim2 = row.get('Dim2', 0)
                max_s = row.get('Max_Stress', 0)
                self.rf_log(f"  {prop:<12} {dim1:>8.2f} {dim2:>8.2f} {max_s:>12.4f}")

            # Save files
            merged_path = os.path.join(out_dir, "merged_combined_stress.csv")
            self.merged_combined_df.to_csv(merged_path, index=False)
            self.rf_log(f"\n  Saved: merged_combined_stress.csv ({len(self.merged_combined_df)} rows)")

            summary_path = os.path.join(out_dir, "max_stress_summary.csv")
            max_stress_summary.to_csv(summary_path, index=False)
            self.rf_log(f"  Saved: max_stress_summary.csv ({n_props} rows)")

            elem_summary_path = os.path.join(out_dir, "max_stress_element_summary.csv")
            max_stress_elem_summary.to_csv(elem_summary_path, index=False)
            self.rf_log(f"  Saved: max_stress_element_summary.csv ({n_elems} rows)")

            self.merge_status.config(
                text=f"✓ Props: {n_props}, Elems: {n_elems}",
                foreground="green"
            )

            self.rf_update_progress(100, "Done")

        except Exception as e:
            self.rf_log(f"\nERROR: {e}")
            import traceback
            self.rf_log(traceback.format_exc())
            self.merge_status.config(text=f"Error: {e}", foreground="red")
            messagebox.showerror("Error", str(e))
    
    def rf_load_allowable(self):
        """Load Allowable stress data and fit power law."""
        path = self.allowable_path.get()
        if not path or not os.path.exists(path):
            messagebox.showerror("Error", "Select a valid Allowable file")
            return
        
        self.rf_log("\n" + "="*70)
        self.rf_log("LOADING ALLOWABLE DATA & FITTING POWER LAW")
        self.rf_log("="*70)
        
        try:
            r2_threshold = float(self.r2_threshold_var.get())
            min_data_pts = int(self.min_data_points_var.get())
        except:
            r2_threshold = 0.95
            min_data_pts = 3
        
        self.rf_log(f"R² Threshold: {r2_threshold}, Min Data Points: {min_data_pts}")
        
        self.rf_update_progress(10, "Loading allowable file...")
        
        try:
            # Load file
            if path.endswith('.csv'):
                raw_df = pd.read_csv(path)
            else:
                xl = pd.ExcelFile(path)
                bar_sheet = None
                for sheet in xl.sheet_names:
                    if 'bar' in sheet.lower() or 'allowable' in sheet.lower():
                        bar_sheet = sheet
                        break
                raw_df = pd.read_excel(path, sheet_name=bar_sheet if bar_sheet else 0)
            
            self.rf_log(f"\nLoaded: {len(raw_df)} rows")
            self.rf_log(f"Columns: {list(raw_df.columns)}")
            
            # Clean column names
            clean_cols = {}
            for col in raw_df.columns:
                clean_name = col.replace('\n', ' ').replace('\r', ' ').strip()
                clean_name = ' '.join(clean_name.split())
                clean_cols[col] = clean_name
            raw_df = raw_df.rename(columns=clean_cols)
            
            # Map column names
            col_map = {}
            for col in raw_df.columns:
                col_up = col.upper().replace(' ', '_').replace('BAR_', '').replace('(MM)', '').strip('_')
                
                if col_up in ['PROPERTY_ID', 'PROPERTY', 'PROP_ID']:
                    col_map[col] = 'Property'
                elif col_up in ['ELEMENT_ID', 'ELEMENT']:
                    col_map[col] = 'Element_ID'
                elif col_up in ['ELEMENT_TYPE', 'ELEMENT_TYP']:
                    col_map[col] = 'Element_Type'
                elif col_up in ['T', 'THICKNESS', 'T_MM']:
                    col_map[col] = 'Thickness'
                elif col_up in ['ALLOWABLE', 'ALLOW', 'ALLOWABLE_STRESS']:
                    col_map[col] = 'Allowable'
            
            df = raw_df.rename(columns=col_map)

            # Save full df for element-based fitting (before Property grouping)
            df_full_elements = df.copy() if 'Element_ID' in df.columns else None

            # Check for NEW format
            if 'Element_Type' in df.columns or 'Element_ID' in df.columns:
                self.rf_log("\nDetected NEW format with Element_Type/Element_ID")
                df = self._process_new_allowable_format(df)

            self.allowable_df = df
            
            self.rf_update_progress(40, "Fitting power law curves...")
            
            # Convert to numeric
            df['Thickness'] = pd.to_numeric(df['Thickness'], errors='coerce')
            df['Allowable'] = pd.to_numeric(df['Allowable'], errors='coerce')
            df['Property'] = pd.to_numeric(df['Property'], errors='coerce')
            df = df.dropna(subset=['Property', 'Thickness', 'Allowable'])
            
            properties = df['Property'].unique()
            self.rf_log(f"\nFitting {len(properties)} properties...")
            
            self.allowable_interp = {}
            excluded_r2 = []
            excluded_data = []
            valid_props = []
            
            for pid in properties:
                pid_int = int(pid)
                prop_data = df[df['Property'] == pid]
                n_pts = len(prop_data)
                
                if n_pts < min_data_pts:
                    avg = prop_data['Allowable'].mean()
                    self.allowable_interp[pid_int] = {'a': avg, 'b': 0, 'n_pts': n_pts, 'r2': 0, 'excluded': True}
                    excluded_data.append((pid_int, n_pts))
                    continue
                
                try:
                    x = prop_data['Thickness'].values.astype(float)
                    y = prop_data['Allowable'].values.astype(float)
                    
                    valid = (x > 0) & (y > 0)
                    x, y = x[valid], y[valid]
                    
                    if len(x) < 2:
                        self.allowable_interp[pid_int] = {'a': np.mean(y), 'b': 0, 'n_pts': len(x), 'r2': 0, 'excluded': True}
                        excluded_data.append((pid_int, len(x)))
                        continue
                    
                    # Power law fit
                    log_x, log_y = np.log(x), np.log(y)
                    coeffs = np.polyfit(log_x, log_y, 1)
                    b, log_a = coeffs[0], coeffs[1]
                    a = np.exp(log_a)
                    
                    # R²
                    y_pred = a * (x ** b)
                    ss_res = np.sum((y - y_pred) ** 2)
                    ss_tot = np.sum((y - np.mean(y)) ** 2)
                    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
                    
                    if r2 < r2_threshold:
                        self.allowable_interp[pid_int] = {'a': np.mean(y), 'b': 0, 'n_pts': n_pts, 'r2': r2, 'excluded': True}
                        excluded_r2.append((pid_int, r2, n_pts))
                    else:
                        self.allowable_interp[pid_int] = {'a': a, 'b': b, 'n_pts': n_pts, 'r2': r2, 'excluded': False}
                        valid_props.append(pid_int)
                    
                except Exception as e:
                    self.allowable_interp[pid_int] = {'a': 100, 'b': 0, 'n_pts': n_pts, 'r2': 0, 'excluded': True}
                    excluded_data.append((pid_int, n_pts))
            
            # Report
            self.rf_log(f"\n{'='*50}")
            self.rf_log(f"Valid fits (R² >= {r2_threshold}): {len(valid_props)}")
            self.rf_log(f"Excluded (R² < {r2_threshold}): {len(excluded_r2)}")
            self.rf_log(f"Excluded (data < {min_data_pts}): {len(excluded_data)}")
            
            if valid_props:
                self.rf_log(f"\nSample valid fits (Property):")
                for pid in valid_props[:5]:
                    p = self.allowable_interp[pid]
                    self.rf_log(f"  Property {pid}: Allow = {p['a']:.4f} × T^({p['b']:.4f}), R²={p['r2']:.4f}")

            # =========================================================
            # ELEMENT-BASED CURVE FITTING
            # =========================================================
            self.rf_update_progress(70, "Fitting element-based curves...")
            self.allowable_elem_interp = {}

            if df_full_elements is not None:
                self.rf_log(f"\n{'='*50}")
                self.rf_log("ELEMENT-BASED CURVE FITTING")
                self.rf_log(f"{'='*50}")

                # Convert to numeric
                df_elem = df_full_elements.copy()
                df_elem['Thickness'] = pd.to_numeric(df_elem['Thickness'], errors='coerce')
                df_elem['Allowable'] = pd.to_numeric(df_elem['Allowable'], errors='coerce')
                df_elem['Element_ID'] = pd.to_numeric(df_elem['Element_ID'], errors='coerce')
                df_elem['Property'] = pd.to_numeric(df_elem['Property'], errors='coerce')
                df_elem = df_elem.dropna(subset=['Element_ID', 'Thickness', 'Allowable'])

                elements = df_elem['Element_ID'].unique()
                self.rf_log(f"Fitting {len(elements)} elements...")

                excluded_elem_r2 = []
                excluded_elem_data = []
                valid_elems = []

                for elem_id in elements:
                    elem_int = int(elem_id)
                    elem_data = df_elem[df_elem['Element_ID'] == elem_id].copy()

                    # Get property for this element
                    elem_pid = elem_data['Property'].iloc[0] if len(elem_data) > 0 else None
                    elem_pid_int = int(elem_pid) if pd.notna(elem_pid) else None

                    # =================================================
                    # CRITICAL: Filter by Element_Type first
                    # 1. Sort by Allowable (smallest to largest)
                    # 2. Find the Element_Type with lowest allowable
                    # 3. Filter by that Element_Type
                    # 4. For each Thickness, take minimum Allowable
                    # =================================================
                    if 'Element_Type' in elem_data.columns and elem_data['Element_Type'].notna().any():
                        # Sort by Allowable ascending
                        elem_data_sorted = elem_data.sort_values('Allowable', ascending=True)

                        # Get the critical Element_Type (the one with lowest allowable)
                        critical_elem_type = elem_data_sorted.iloc[0]['Element_Type']

                        # Filter by critical Element_Type
                        filtered_elem_data = elem_data[elem_data['Element_Type'] == critical_elem_type].copy()

                        # For each unique Thickness, get minimum Allowable
                        fit_data = []
                        for t in sorted(filtered_elem_data['Thickness'].unique()):
                            t_data = filtered_elem_data[filtered_elem_data['Thickness'] == t]
                            min_allow = t_data['Allowable'].min()
                            fit_data.append({'Thickness': float(t), 'Allowable': float(min_allow)})

                        fit_df = pd.DataFrame(fit_data)
                    else:
                        # No Element_Type column - use all data, take min per thickness
                        fit_data = []
                        for t in sorted(elem_data['Thickness'].unique()):
                            t_data = elem_data[elem_data['Thickness'] == t]
                            min_allow = t_data['Allowable'].min()
                            fit_data.append({'Thickness': float(t), 'Allowable': float(min_allow)})

                        fit_df = pd.DataFrame(fit_data)

                    n_pts = len(fit_df)

                    if n_pts < min_data_pts:
                        avg = fit_df['Allowable'].mean() if len(fit_df) > 0 else 0
                        self.allowable_elem_interp[elem_int] = {
                            'a': avg, 'b': 0, 'n_pts': n_pts, 'r2': 0,
                            'excluded': True, 'property': elem_pid_int
                        }
                        excluded_elem_data.append((elem_int, n_pts))
                        continue

                    try:
                        x = fit_df['Thickness'].values.astype(float)
                        y = fit_df['Allowable'].values.astype(float)

                        valid_mask = (x > 0) & (y > 0)
                        x, y = x[valid_mask], y[valid_mask]

                        if len(x) < 2:
                            self.allowable_elem_interp[elem_int] = {
                                'a': np.mean(y), 'b': 0, 'n_pts': len(x), 'r2': 0,
                                'excluded': True, 'property': elem_pid_int
                            }
                            excluded_elem_data.append((elem_int, len(x)))
                            continue

                        # Power law fit: log_y = b * log_x + log_a
                        log_x, log_y = np.log(x), np.log(y)
                        coeffs = np.polyfit(log_x, log_y, 1)
                        b, log_a = coeffs[0], coeffs[1]
                        a = np.exp(log_a)

                        # Calculate R²
                        y_pred = a * (x ** b)
                        ss_res = np.sum((y - y_pred) ** 2)
                        ss_tot = np.sum((y - np.mean(y)) ** 2)
                        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

                        if r2 < r2_threshold:
                            self.allowable_elem_interp[elem_int] = {
                                'a': np.mean(y), 'b': 0, 'n_pts': n_pts, 'r2': r2,
                                'excluded': True, 'property': elem_pid_int
                            }
                            excluded_elem_r2.append((elem_int, r2, n_pts))
                        else:
                            self.allowable_elem_interp[elem_int] = {
                                'a': a, 'b': b, 'n_pts': n_pts, 'r2': r2,
                                'excluded': False, 'property': elem_pid_int
                            }
                            valid_elems.append(elem_int)

                    except Exception as e:
                        self.allowable_elem_interp[elem_int] = {
                            'a': 100, 'b': 0, 'n_pts': n_pts, 'r2': 0,
                            'excluded': True, 'property': elem_pid_int
                        }
                        excluded_elem_data.append((elem_int, n_pts))

                # Element fitting report
                self.rf_log(f"Valid element fits (R² >= {r2_threshold}): {len(valid_elems)}")
                self.rf_log(f"Excluded elements (R² < {r2_threshold}): {len(excluded_elem_r2)}")
                self.rf_log(f"Excluded elements (data < {min_data_pts}): {len(excluded_elem_data)}")

                if valid_elems:
                    self.rf_log(f"\nSample valid fits (Element):")
                    for elem_id in valid_elems[:5]:
                        e = self.allowable_elem_interp[elem_id]
                        self.rf_log(f"  Element {elem_id}: Allow = {e['a']:.4f} × T^({e['b']:.4f}), R²={e['r2']:.4f}")

                total_elem_excluded = len(excluded_elem_r2) + len(excluded_elem_data)
                self.allowable_status.config(
                    text=f"✓ Prop: {len(valid_props)} valid | Elem: {len(valid_elems)} valid, {total_elem_excluded} excl",
                    foreground="green"
                )
            else:
                self.rf_log("\nNo Element_ID column found - skipping element-based fitting")
                self.allowable_status.config(
                    text=f"✓ Loaded: {len(valid_props)} valid, {len(excluded_r2)+len(excluded_data)} excluded",
                    foreground="green"
                )

            self.rf_update_progress(100, "Done")
            
        except Exception as e:
            self.rf_log(f"\nError: {e}")
            import traceback
            self.rf_log(traceback.format_exc())
            self.allowable_status.config(text=f"Error: {e}", foreground="red")
            messagebox.showerror("Error", str(e))
    
    def _process_new_allowable_format(self, df):
        """Process NEW format with Element_Type."""
        self.rf_log("  Processing NEW format...")
        
        result_data = []
        properties = df['Property'].unique()
        
        for pid in properties:
            prop_df = df[df['Property'] == pid].copy()
            if len(prop_df) == 0:
                continue
            
            prop_df_sorted = prop_df.sort_values('Allowable', ascending=True)
            critical_row = prop_df_sorted.iloc[0]
            
            crit_elem_id = critical_row.get('Element_ID', None)
            crit_elem_type = critical_row.get('Element_Type', None)
            
            if crit_elem_id is not None and crit_elem_type is not None:
                try:
                    crit_id = int(crit_elem_id)
                    mask = (prop_df['Element_ID'].astype(float).astype(int) == crit_id) & \
                           (prop_df['Element_Type'] == crit_elem_type)
                    filtered_df = prop_df[mask].copy()
                except:
                    filtered_df = prop_df.copy()
            elif crit_elem_type is not None:
                filtered_df = prop_df[prop_df['Element_Type'] == crit_elem_type].copy()
            else:
                filtered_df = prop_df.copy()
            
            for t in sorted(filtered_df['Thickness'].unique()):
                t_data = filtered_df[filtered_df['Thickness'] == t]
                min_allow = t_data['Allowable'].min()
                result_data.append({'Property': int(pid), 'Thickness': float(t), 'Allowable': float(min_allow)})
        
        self.rf_log(f"  Processed: {len(properties)} properties -> {len(result_data)} data points")
        return pd.DataFrame(result_data)
    
    def get_allowable_stress(self, pid, thickness):
        """Get allowable stress for property at thickness using power law."""
        pid_int = int(pid) if not isinstance(pid, int) else pid

        if pid_int not in self.allowable_interp:
            return None

        params = self.allowable_interp[pid_int]

        if params.get('excluded', False) and params['b'] == 0:
            return params['a']

        return params['a'] * (thickness ** params['b'])

    def get_allowable_stress_elem(self, elem_id, thickness):
        """Get allowable stress for element at thickness using element's own power law fit."""
        elem_int = int(elem_id) if not isinstance(elem_id, int) else elem_id

        if elem_int not in self.allowable_elem_interp:
            return None

        params = self.allowable_elem_interp[elem_int]

        if params.get('excluded', False) and params['b'] == 0:
            return params['a']

        return params['a'] * (thickness ** params['b'])

    def get_required_thickness_elem(self, elem_id, target_stress, min_rf_target=1.0):
        """
        Calculate required thickness for element to achieve target RF.
        Uses element's own curve fit parameters.
        """
        elem_int = int(elem_id) if not isinstance(elem_id, int) else elem_id

        if elem_int not in self.allowable_elem_interp:
            return None

        params = self.allowable_elem_interp[elem_int]
        a = params['a']
        b = params['b']

        if params.get('excluded', False) or b == 0:
            return None

        required_allowable = abs(target_stress) * min_rf_target

        if a <= 0:
            return None

        try:
            ratio = required_allowable / a
            if ratio <= 0:
                return None

            if b != 0:
                t_required = ratio ** (1.0 / b)
                if t_required > 0 and t_required < 1000:
                    return t_required
            return None
        except Exception:
            return None

    def get_required_thickness(self, pid, target_stress, min_rf_target=1.0):
        """
        Calculate required thickness to achieve target RF.
        
        From: Allowable = a × T^b
        For RF >= min_rf_target: Allowable >= target_stress × min_rf_target
        So: a × T^b >= target_stress × min_rf_target
        T >= (target_stress × min_rf_target / a)^(1/b)
        
        Returns:
            Required thickness or None if cannot calculate
        """
        pid_int = int(pid) if not isinstance(pid, int) else pid
        
        if pid_int not in self.allowable_interp:
            return None
        
        params = self.allowable_interp[pid_int]
        a = params['a']
        b = params['b']
        
        # If excluded or b=0, cannot calculate (constant allowable)
        if params.get('excluded', False) or b == 0:
            return None
        
        # Required allowable to meet RF target
        required_allowable = abs(target_stress) * min_rf_target
        
        if a <= 0:
            return None
        
        # T_required = (required_allowable / a)^(1/b)
        try:
            ratio = required_allowable / a
            
            if ratio <= 0:
                return None
            
            # For negative b (typical case: allowable decreases with thickness)
            # ratio > 1 means we need smaller thickness
            # ratio < 1 means we need larger thickness
            
            if b != 0:
                t_required = ratio ** (1.0 / b)
                
                # Sanity check: thickness should be positive and reasonable
                if t_required > 0 and t_required < 1000:
                    return t_required
                else:
                    return None
            else:
                return None
                
        except Exception:
            return None
    
    def rf_calculate_rf(self):
        """Calculate Reserve Factor for each property and element."""
        self.rf_log("\n" + "="*70)
        self.rf_log("CALCULATING RESERVE FACTORS + REQUIRED THICKNESS")
        self.rf_log("="*70)

        if self.max_stress_df is None:
            messagebox.showerror("Error", "Create max stress summary first (Step 2)")
            return

        if not self.allowable_interp:
            messagebox.showerror("Error", "Load allowable data first (Step 3)")
            return

        try:
            min_rf_target = float(self.min_rf_var.get())
        except:
            min_rf_target = 1.0

        self.rf_log(f"Target RF: {min_rf_target}")

        self.rf_update_progress(10, "Calculating RF (Property-based)...")

        try:
            # =========================================================
            # PART A: Property-based RF calculation
            # =========================================================
            self.rf_log("\n--- Property-based RF ---")
            results = []

            for idx, row in self.max_stress_df.iterrows():
                pid = row.get('Property')
                dim1 = row.get('Dim1', 0)
                dim2 = row.get('Dim2', 0)
                max_stress = abs(row.get('Max_Stress', 0))
                min_stress = row.get('Min_Stress', 0)
                element = row.get('Element', None)
                combined_lc = row.get('Combined_LC', None)

                pid_int = int(pid) if pd.notna(pid) else None

                allowable = self.get_allowable_stress(pid_int, dim1) if pid_int else None

                if allowable and max_stress > 0:
                    rf = allowable / max_stress
                elif max_stress == 0:
                    rf = 999.0
                else:
                    rf = 0.0

                if allowable is None:
                    status = "NO_ALLOWABLE"
                elif rf >= min_rf_target:
                    status = "PASS"
                else:
                    status = "FAIL"

                is_excluded = False
                if pid_int and pid_int in self.allowable_interp:
                    is_excluded = self.allowable_interp[pid_int].get('excluded', False)

                # Calculate Required Thickness
                required_thickness = None
                if pid_int and max_stress > 0:
                    required_thickness = self.get_required_thickness(pid_int, max_stress, min_rf_target)

                results.append({
                    'Property': pid_int,
                    'Dim1': dim1,
                    'Dim2': dim2,
                    'Max_Stress': max_stress,
                    'Min_Stress': min_stress,
                    'Allowable': allowable,
                    'RF': rf,
                    'Status': status,
                    'Required_Thickness': required_thickness,
                    'Element': element,
                    'Combined_LC': combined_lc,
                    'Allowable_Excluded': is_excluded
                })

            self.rf_results_df = pd.DataFrame(results)

            # Statistics for Property-based
            valid = self.rf_results_df[self.rf_results_df['Status'] != 'NO_ALLOWABLE']

            n_total = len(self.rf_results_df)
            n_pass = len(self.rf_results_df[self.rf_results_df['Status'] == 'PASS'])
            n_fail = len(self.rf_results_df[self.rf_results_df['Status'] == 'FAIL'])
            n_no_allow = len(self.rf_results_df[self.rf_results_df['Status'] == 'NO_ALLOWABLE'])

            min_rf = valid['RF'].min() if len(valid) > 0 else 0
            max_rf = valid['RF'].max() if len(valid) > 0 else 0
            mean_rf = valid['RF'].mean() if len(valid) > 0 else 0

            self.rf_log(f"  Properties: {n_total}, PASS: {n_pass}, FAIL: {n_fail}, NO_ALLOW: {n_no_allow}")
            self.rf_log(f"  Min RF: {min_rf:.4f}, Max RF: {max_rf:.4f}, Mean RF: {mean_rf:.4f}")

            # =========================================================
            # PART B: Element-based RF calculation
            # =========================================================
            self.rf_update_progress(50, "Calculating RF (Element-based)...")
            self.rf_log("\n--- Element-based RF (using element's own curve fit) ---")

            if hasattr(self, 'max_stress_elem_df') and self.max_stress_elem_df is not None:
                results_elem = []
                elem_fit_count = 0
                prop_fit_count = 0

                for idx, row in self.max_stress_elem_df.iterrows():
                    elem = row.get('Element')
                    pid = row.get('Property')
                    dim1 = row.get('Dim1', 0)
                    dim2 = row.get('Dim2', 0)
                    max_stress = abs(row.get('Max_Stress', 0))
                    min_stress = row.get('Min_Stress', 0)
                    combined_lc = row.get('Combined_LC', None)

                    elem_int = int(elem) if pd.notna(elem) else None
                    pid_int = int(pid) if pd.notna(pid) else None

                    # Try element's own curve fit first, fall back to property
                    allowable = None
                    fit_source = "none"
                    is_excluded = False

                    if elem_int and elem_int in self.allowable_elem_interp:
                        allowable = self.get_allowable_stress_elem(elem_int, dim1)
                        if allowable is not None:
                            fit_source = "element"
                            elem_fit_count += 1
                            is_excluded = self.allowable_elem_interp[elem_int].get('excluded', False)

                    if allowable is None and pid_int and pid_int in self.allowable_interp:
                        allowable = self.get_allowable_stress(pid_int, dim1)
                        if allowable is not None:
                            fit_source = "property"
                            prop_fit_count += 1
                            is_excluded = self.allowable_interp[pid_int].get('excluded', False)

                    if allowable and max_stress > 0:
                        rf = allowable / max_stress
                    elif max_stress == 0:
                        rf = 999.0
                    else:
                        rf = 0.0

                    if allowable is None:
                        status = "NO_ALLOWABLE"
                    elif rf >= min_rf_target:
                        status = "PASS"
                    else:
                        status = "FAIL"

                    # Calculate Required Thickness using element fit if available
                    required_thickness = None
                    if max_stress > 0:
                        if fit_source == "element" and elem_int:
                            required_thickness = self.get_required_thickness_elem(elem_int, max_stress, min_rf_target)
                        elif fit_source == "property" and pid_int:
                            required_thickness = self.get_required_thickness(pid_int, max_stress, min_rf_target)

                    results_elem.append({
                        'Element': elem_int,
                        'Property': pid_int,
                        'Dim1': dim1,
                        'Dim2': dim2,
                        'Max_Stress': max_stress,
                        'Min_Stress': min_stress,
                        'Allowable': allowable,
                        'RF': rf,
                        'Status': status,
                        'Required_Thickness': required_thickness,
                        'Combined_LC': combined_lc,
                        'Allowable_Excluded': is_excluded,
                        'Fit_Source': fit_source
                    })

                self.rf_results_elem_df = pd.DataFrame(results_elem)

                # Statistics for Element-based
                valid_elem = self.rf_results_elem_df[self.rf_results_elem_df['Status'] != 'NO_ALLOWABLE']

                n_total_elem = len(self.rf_results_elem_df)
                n_pass_elem = len(self.rf_results_elem_df[self.rf_results_elem_df['Status'] == 'PASS'])
                n_fail_elem = len(self.rf_results_elem_df[self.rf_results_elem_df['Status'] == 'FAIL'])
                n_no_allow_elem = len(self.rf_results_elem_df[self.rf_results_elem_df['Status'] == 'NO_ALLOWABLE'])

                min_rf_elem = valid_elem['RF'].min() if len(valid_elem) > 0 else 0

                self.rf_log(f"  Elements: {n_total_elem}, PASS: {n_pass_elem}, FAIL: {n_fail_elem}, NO_ALLOW: {n_no_allow_elem}")
                self.rf_log(f"  Min RF: {min_rf_elem:.4f}")
                self.rf_log(f"  Fit source: {elem_fit_count} element fits, {prop_fit_count} property fits")
            else:
                self.rf_log("  Element-based summary not available (run Merge & Create Summary first)")
                self.rf_results_elem_df = None

            # =========================================================
            # Show failing properties with Required Thickness
            # =========================================================
            self.rf_log(f"\n{'='*50}")
            failing = self.rf_results_df[self.rf_results_df['Status'] == 'FAIL'].sort_values('RF')
            if len(failing) > 0:
                self.rf_log(f"\n✗ FAILING PROPERTIES ({len(failing)}):")
                self.rf_log(f"{'Property':<12} {'Dim1':>8} {'Stress':>12} {'Allowable':>12} {'RF':>8} {'Req_Thick':>10}")
                self.rf_log("-"*75)
                for _, row in failing.head(10).iterrows():
                    req_t = row['Required_Thickness']
                    req_t_str = f"{req_t:.2f}" if pd.notna(req_t) else "N/A"
                    self.rf_log(f"{row['Property']:<12} {row['Dim1']:>8.2f} {row['Max_Stress']:>12.4f} "
                            f"{row['Allowable']:>12.4f} {row['RF']:>8.4f} {req_t_str:>10}")

            # Show Required Thickness summary
            has_req_thick = self.rf_results_df['Required_Thickness'].notna().sum()
            self.rf_log(f"\nRequired Thickness calculated for: {has_req_thick}/{n_total} properties")

            # Update summary
            pass_pct = (n_pass / n_total * 100) if n_total > 0 else 0
            self.result_summary.config(
                text=f"Min RF = {min_rf:.4f} | Props: {n_pass}/{n_total} PASS ({pass_pct:.1f}%) | {n_fail} FAIL",
                foreground="green" if n_fail == 0 else "red"
            )

            self.rf_update_progress(100, "Done")

        except Exception as e:
            self.rf_log(f"\nError: {e}")
            import traceback
            self.rf_log(traceback.format_exc())
            messagebox.showerror("Error", str(e))
    
    def rf_export_results(self):
        """Export all results with separate Property and Element sheets."""
        if self.rf_results_df is None:
            messagebox.showerror("Error", "Calculate RF first")
            return

        out_dir = self.output_folder.get()
        if not out_dir:
            out_dir = filedialog.askdirectory(title="Select Output Folder")
            if not out_dir:
                return
            self.output_folder.set(out_dir)

        os.makedirs(out_dir, exist_ok=True)

        self.rf_log("\n" + "="*70)
        self.rf_log("EXPORTING RESULTS")
        self.rf_log("="*70)

        try:
            # RF Results - Property based
            rf_path = os.path.join(out_dir, "rf_results_prop.csv")
            self.rf_results_df.to_csv(rf_path, index=False)
            self.rf_log(f"  ✓ rf_results_prop.csv ({len(self.rf_results_df)} rows)")

            # RF Results - Element based
            if hasattr(self, 'rf_results_elem_df') and self.rf_results_elem_df is not None:
                rf_elem_path = os.path.join(out_dir, "rf_results_elem.csv")
                self.rf_results_elem_df.to_csv(rf_elem_path, index=False)
                self.rf_log(f"  ✓ rf_results_elem.csv ({len(self.rf_results_elem_df)} rows)")

            # Max Stress Summary - Property based
            if self.max_stress_df is not None:
                max_path = os.path.join(out_dir, "max_stress_summary.csv")
                self.max_stress_df.to_csv(max_path, index=False)
                self.rf_log(f"  ✓ max_stress_summary.csv ({len(self.max_stress_df)} rows)")

            # Max Stress Summary - Element based
            if hasattr(self, 'max_stress_elem_df') and self.max_stress_elem_df is not None:
                max_elem_path = os.path.join(out_dir, "max_stress_element_summary.csv")
                self.max_stress_elem_df.to_csv(max_elem_path, index=False)
                self.rf_log(f"  ✓ max_stress_element_summary.csv ({len(self.max_stress_elem_df)} rows)")

            # =========================================================
            # Allowable Summary - Property based (curve fit parameters)
            # =========================================================
            allow_prop_data = []
            if self.allowable_interp:
                for pid, p in self.allowable_interp.items():
                    allow_prop_data.append({
                        'Property': pid,
                        'a': p['a'],
                        'b': p['b'],
                        'R2': p['r2'],
                        'n_pts': p['n_pts'],
                        'Excluded': p.get('excluded', False),
                        'Formula': f"Allowable = {p['a']:.4f} × T^({p['b']:.4f})" if not p.get('excluded', False) else f"Allowable = {p['a']:.4f} (constant)"
                    })
                pd.DataFrame(allow_prop_data).to_csv(os.path.join(out_dir, "allowable_fits_props.csv"), index=False)
                self.rf_log(f"  ✓ allowable_fits_props.csv ({len(allow_prop_data)} properties)")

            # =========================================================
            # Element Curve Fit Parameters (a, b, R² for each element)
            # =========================================================
            elem_curve_fit_data = []
            if self.allowable_elem_interp:
                for elem_id, e in self.allowable_elem_interp.items():
                    elem_curve_fit_data.append({
                        'Element': elem_id,
                        'Property': e.get('property'),
                        'a': e['a'],
                        'b': e['b'],
                        'R2': e['r2'],
                        'n_pts': e['n_pts'],
                        'Excluded': e.get('excluded', False),
                        'Formula': f"Allowable = {e['a']:.4f} × T^({e['b']:.4f})" if not e.get('excluded', False) else f"Allowable = {e['a']:.4f} (constant)"
                    })
                pd.DataFrame(elem_curve_fit_data).to_csv(os.path.join(out_dir, "element_curve_fits.csv"), index=False)
                self.rf_log(f"  ✓ element_curve_fits.csv ({len(elem_curve_fit_data)} elements)")

            # =========================================================
            # Allowable Summary - Element based (using element's own curve fit)
            # =========================================================
            allow_elem_data = []
            if hasattr(self, 'max_stress_elem_df') and self.max_stress_elem_df is not None:
                elem_fit_used = 0
                prop_fit_used = 0

                for _, row in self.max_stress_elem_df.iterrows():
                    elem = row.get('Element')
                    pid = row.get('Property')
                    dim1 = row.get('Dim1', 0)

                    elem_int = int(elem) if pd.notna(elem) else None
                    pid_int = int(pid) if pd.notna(pid) else None

                    # Calculate allowable - try element fit first, then property fit
                    allowable = None
                    formula = "N/A"
                    excluded = False
                    a_val = b_val = r2_val = None
                    fit_source = "none"

                    # First try element's own curve fit
                    if elem_int and elem_int in self.allowable_elem_interp:
                        e = self.allowable_elem_interp[elem_int]
                        a_val = e['a']
                        b_val = e['b']
                        r2_val = e['r2']
                        excluded = e.get('excluded', False)
                        fit_source = "element"
                        elem_fit_used += 1

                        if excluded:
                            allowable = a_val
                            formula = f"Allowable = {a_val:.4f} (constant, elem fit)"
                        else:
                            if dim1 > 0:
                                allowable = a_val * (dim1 ** b_val)
                                formula = f"{a_val:.4f} × {dim1:.4f}^({b_val:.4f}) = {allowable:.4f} (elem fit)"

                    # Fall back to property fit if no element fit
                    elif pid_int and pid_int in self.allowable_interp:
                        p = self.allowable_interp[pid_int]
                        a_val = p['a']
                        b_val = p['b']
                        r2_val = p['r2']
                        excluded = p.get('excluded', False)
                        fit_source = "property"
                        prop_fit_used += 1

                        if excluded:
                            allowable = a_val
                            formula = f"Allowable = {a_val:.4f} (constant, prop fit)"
                        else:
                            if dim1 > 0:
                                allowable = a_val * (dim1 ** b_val)
                                formula = f"{a_val:.4f} × {dim1:.4f}^({b_val:.4f}) = {allowable:.4f} (prop fit)"

                    allow_elem_data.append({
                        'Element': elem_int,
                        'Property': pid_int,
                        'Dim1': dim1,
                        'a': a_val,
                        'b': b_val,
                        'R2': r2_val,
                        'Excluded': excluded,
                        'Allowable': allowable,
                        'Formula': formula,
                        'Fit_Source': fit_source
                    })

                pd.DataFrame(allow_elem_data).to_csv(os.path.join(out_dir, "allowable_fits_elems.csv"), index=False)
                self.rf_log(f"  ✓ allowable_fits_elems.csv ({len(allow_elem_data)} elements)")
                self.rf_log(f"    ({elem_fit_used} element fits, {prop_fit_used} property fits)")

            # =========================================================
            # Excel Summary with separate sheets
            # =========================================================
            try:
                excel_path = os.path.join(out_dir, "rf_check_summary.xlsx")
                with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                    # RF Results sheets
                    self.rf_results_df.to_excel(writer, sheet_name='RF_Results_Prop', index=False)
                    if hasattr(self, 'rf_results_elem_df') and self.rf_results_elem_df is not None:
                        self.rf_results_elem_df.to_excel(writer, sheet_name='RF_Results_Elem', index=False)

                    # Max Stress sheets
                    if self.max_stress_df is not None:
                        self.max_stress_df.to_excel(writer, sheet_name='Max_Stress_Prop', index=False)
                    if hasattr(self, 'max_stress_elem_df') and self.max_stress_elem_df is not None:
                        self.max_stress_elem_df.to_excel(writer, sheet_name='Max_Stress_Elem', index=False)

                    # Allowable Fits sheets
                    if allow_prop_data:
                        pd.DataFrame(allow_prop_data).to_excel(writer, sheet_name='Allowable_Fits_Props', index=False)
                    if elem_curve_fit_data:
                        pd.DataFrame(elem_curve_fit_data).to_excel(writer, sheet_name='Elem_Curve_Fits', index=False)
                    if allow_elem_data:
                        pd.DataFrame(allow_elem_data).to_excel(writer, sheet_name='Allowable_Fits_Elems', index=False)

                self.rf_log(f"  ✓ rf_check_summary.xlsx")
                self.rf_log(f"    Sheets: RF_Results_Prop, RF_Results_Elem, Max_Stress_Prop, Max_Stress_Elem,")
                self.rf_log(f"            Allowable_Fits_Props, Elem_Curve_Fits, Allowable_Fits_Elems")
            except Exception as ex:
                self.rf_log(f"  Warning: Excel export failed: {ex}")

            self.rf_log(f"\nExported to: {out_dir}")
            messagebox.showinfo("Success", f"Results exported to:\n{out_dir}")

        except Exception as e:
            self.rf_log(f"\nError: {e}")
            import traceback
            self.rf_log(traceback.format_exc())
            messagebox.showerror("Error", str(e))



def main():
    root = tk.Tk()
    app = IntegratedBDFRFTool(root)
    root.mainloop()


if __name__ == "__main__":
    main()
